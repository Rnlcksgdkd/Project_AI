{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 나무위키 덤프 + 포켓몬 크롤링 데이터 Kor2Vec입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "from konlpy.tag import Twitter\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(train_text, min_count, sampling_rate):\n",
    "    words = list()\n",
    "        #워드-인덱스\n",
    "        #인덱스-워드\n",
    "        #형태소-인덱스\n",
    "        #인덱스-형태소\n",
    "        #총 4개의 리스트 생성\n",
    "    for line in desc_list:\n",
    "        sentence = re.sub(r\"[^ㄱ-힣a-zA-Z0-9]+\", ' ', line).strip().split()\n",
    "        if sentence:\n",
    "            words.append(sentence)\n",
    "\n",
    "    word_counter = [['UNK', -1]] #시작점\n",
    "    word_counter.extend(collections.Counter([word for sentence in words for word in sentence]).most_common())\n",
    "    word_counter = [item for item in word_counter if item[1] >= min_count or item[0] == 'UNK']\n",
    "    #단어의 빈도수가 적은 것은 삭제된다\n",
    "\n",
    "    word_list = list()\n",
    "    word_dict = dict()\n",
    "    for word, count in word_counter:\n",
    "        word_list.append(word) # 학습에 사용된 word를 저장한다. (visualize를 위해)\n",
    "        word_dict[word] = len(word_dict)\n",
    "    word_reverse_dict = dict(zip(word_dict.values(), word_dict.keys()))\n",
    "\n",
    "    word_to_pos_li = dict() #워드-인덱스\n",
    "    pos_list = list()\n",
    "    twitter = Twitter()\n",
    "    for w in word_dict:\n",
    "        w_pos_li = list()\n",
    "        for pos in twitter.pos(w, norm=True):\n",
    "            w_pos_li.append(pos)\n",
    "\n",
    "        word_to_pos_li[word_dict[w]] = w_pos_li\n",
    "        pos_list += w_pos_li\n",
    "\n",
    "    pos_counter = collections.Counter(pos_list).most_common()\n",
    "\n",
    "    pos_dict = dict()\n",
    "    for pos, _ in pos_counter:\n",
    "        pos_dict[pos] = len(pos_dict)\n",
    "\n",
    "    pos_reverse_dict = dict(zip(pos_dict.values(), pos_dict.keys()))\n",
    "\n",
    "    word_to_pos_dict = dict()\n",
    "\n",
    "    for word_id, pos_li in word_to_pos_li.items():\n",
    "        pos_id_li = list()\n",
    "        for pos in pos_li:\n",
    "            pos_id_li.append(pos_dict[pos])\n",
    "        word_to_pos_dict[word_id] = pos_id_li\n",
    "\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for sentence in words:\n",
    "        s = list()\n",
    "        for word in sentence:\n",
    "            if word in word_dict:\n",
    "                index = word_dict[word]\n",
    "            else:\n",
    "                index = word_dict['UNK']\n",
    "                unk_count += 1\n",
    "            s.append(index)\n",
    "        data.append(s)\n",
    "    word_counter[0][1] = max(1, unk_count)\n",
    "\n",
    "    # data = sub_sampling(data, word_counter, word_dict, sampling_rate)\n",
    "\n",
    "    return data, word_dict, word_reverse_dict, pos_dict, pos_reverse_dict, word_to_pos_dict, word_list\n",
    "\n",
    "def sub_sampling(data, word_counter, word_dict, sampling_rate):\n",
    "    total_words = sum([len(sentence) for sentence in data])\n",
    "    # print(\"total_words: {}\".format(total_words))\n",
    "    prob_dict = dict()\n",
    "    for word, count in word_counter:\n",
    "        f = count / total_words # 빈도수가 많을수록 f가 1에 가까워짐.\n",
    "        p = max(0, 1 - math.sqrt(sampling_rate / f)) # sampling_rate가 0.0001이면 f가 클수록 prob이 커진다.\n",
    "        prob_dict[word_dict[word]] = p\n",
    "        # print(\"count : {}, f : {}, p : {}, prob_dict : {}\".format(count, f, p, prob_dict))\n",
    "\n",
    "    new_data = list()\n",
    "    for sentence in data:\n",
    "        s = list()\n",
    "        for word in sentence:\n",
    "            prob = prob_dict[word]\n",
    "            if random.random() > prob: # prob이 작을수록 s에 저장되기 쉬움.\n",
    "                s.append(word)\n",
    "        new_data.append(s)\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawling한 데이터를 불러온다.\n",
    "pk_data = pd.read_csv(DATA_PATH + 'pk_data_g1.csv')\n",
    "desc_list = []\n",
    "\n",
    "for i in range(len(pk_data)):\n",
    "    for desc in pk_data['desc'][i].split('.'):\n",
    "        desc_list.append(desc)\n",
    "        \n",
    "fname= './data/namu_wonnie.txt'\n",
    "    \n",
    "with open(fname, 'r') as f:\n",
    "    namu = [data for data in f]\n",
    "\n",
    "for data in namu:\n",
    "    desc_list.append(data)\n",
    "\n",
    "sampling_rate = 0.0001\n",
    "min_count = 5\n",
    "\n",
    "data, word_dict, word_reverse_dict, pos_dict, pos_reverse_dict, word_to_pos_dict, word_list \\\n",
    "        = build_dataset(desc_list, min_count, sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 사용된 word list 저장\n",
    "f = open(\"word_list.txt\", 'w')\n",
    "for word in word_list:\n",
    "    input_word = \"{} \".format(word)\n",
    "    f.write(input_word)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences : 252929\n",
      "vocabulary size : 96117\n",
      "pos size : 37768\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(word_dict)\n",
    "pos_size = len(pos_dict)\n",
    "num_sentences = len(data)\n",
    "\n",
    "print(\"number of sentences :\", num_sentences)\n",
    "print(\"vocabulary size :\", vocabulary_size)\n",
    "print(\"pos size :\", pos_size)\n",
    "\n",
    "pos_li = []\n",
    "for key in sorted(pos_reverse_dict):\n",
    "    pos_li.append(pos_reverse_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "batch_size = 150\n",
    "\n",
    "# kor2vec 의 input index list와 output index list를 만든다.\n",
    "# 윈도우 사이즈에 따라 input output pair가 늘어난다.(input이 중복)\n",
    "def generate_input_output_list(data, window_size):\n",
    "    input_li = list()\n",
    "    output_li = list()\n",
    "    for sentence in data:\n",
    "        for i in range(len(sentence)):\n",
    "            for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
    "                if i != j:\n",
    "                    if sentence[i]!=word_dict['UNK'] and sentence[j]!=word_dict['UNK']:\n",
    "                        input_li.append(sentence[i])\n",
    "                        output_li.append(sentence[j])\n",
    "    return input_li, output_li\n",
    "\n",
    "input_li, output_li = generate_input_output_list(data, window_size)\n",
    "input_li_size = len(input_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "22158178\n",
      "(150,)\n",
      "[20190 20190 20190 20190 20190  1325  1325  1325  1325  1325  1325   734\n",
      "   734   734   734   734   734   734 35774 35774 35774 35774 35774 35774\n",
      " 35774 35774 21020 21020 21020 21020 21020 21020 21020 21020   253   253\n",
      "   253   253   253   253   253   253  1574  1574  1574  1574  1574  1574\n",
      "  1574   165   165   165   165   165   165 16331 16331 16331 16331 16331\n",
      " 38423 38423 38423 38423 38423  1325  1325  1325  1325  1325  1325   734\n",
      "   734   734   734   734   734   734  1539  1539  1539  1539  1539  1539\n",
      "  1539  1539 21020 21020 21020 21020 21020 21020 21020 21020 35775 35775\n",
      " 35775 35775 35775 35775 35775 35775 35775   103   103   103   103   103\n",
      "   103   103   103 17410 17410 17410 17410 17410 17410 17410    47    47\n",
      "    47    47    47    47     7     7     7     7  5552  5552  5552  5552\n",
      "  5552   265   265   265   265   265  2019  2019  2019  2019  2019  8901\n",
      "  8901  8901  8901  8901  1797  1797]\n",
      "(150, 1)\n",
      "[[ 1325]\n",
      " [  734]\n",
      " [35774]\n",
      " [21020]\n",
      " [  253]\n",
      " [20190]\n",
      " [  734]\n",
      " [35774]\n",
      " [21020]\n",
      " [  253]\n",
      " [ 1574]\n",
      " [20190]\n",
      " [ 1325]\n",
      " [35774]\n",
      " [21020]\n",
      " [  253]\n",
      " [ 1574]\n",
      " [  165]\n",
      " [20190]\n",
      " [ 1325]\n",
      " [  734]\n",
      " [21020]\n",
      " [  253]\n",
      " [ 1574]\n",
      " [  165]\n",
      " [16331]\n",
      " [20190]\n",
      " [ 1325]\n",
      " [  734]\n",
      " [35774]\n",
      " [  253]\n",
      " [ 1574]\n",
      " [  165]\n",
      " [16331]\n",
      " [20190]\n",
      " [ 1325]\n",
      " [  734]\n",
      " [35774]\n",
      " [21020]\n",
      " [ 1574]\n",
      " [  165]\n",
      " [16331]\n",
      " [ 1325]\n",
      " [  734]\n",
      " [35774]\n",
      " [21020]\n",
      " [  253]\n",
      " [  165]\n",
      " [16331]\n",
      " [  734]\n",
      " [35774]\n",
      " [21020]\n",
      " [  253]\n",
      " [ 1574]\n",
      " [16331]\n",
      " [35774]\n",
      " [21020]\n",
      " [  253]\n",
      " [ 1574]\n",
      " [  165]\n",
      " [ 1325]\n",
      " [  734]\n",
      " [ 1539]\n",
      " [21020]\n",
      " [35775]\n",
      " [38423]\n",
      " [  734]\n",
      " [ 1539]\n",
      " [21020]\n",
      " [35775]\n",
      " [  103]\n",
      " [38423]\n",
      " [ 1325]\n",
      " [ 1539]\n",
      " [21020]\n",
      " [35775]\n",
      " [  103]\n",
      " [17410]\n",
      " [38423]\n",
      " [ 1325]\n",
      " [  734]\n",
      " [21020]\n",
      " [35775]\n",
      " [  103]\n",
      " [17410]\n",
      " [   47]\n",
      " [38423]\n",
      " [ 1325]\n",
      " [  734]\n",
      " [ 1539]\n",
      " [35775]\n",
      " [  103]\n",
      " [17410]\n",
      " [   47]\n",
      " [38423]\n",
      " [ 1325]\n",
      " [  734]\n",
      " [ 1539]\n",
      " [21020]\n",
      " [  103]\n",
      " [17410]\n",
      " [   47]\n",
      " [    7]\n",
      " [ 1325]\n",
      " [  734]\n",
      " [ 1539]\n",
      " [21020]\n",
      " [35775]\n",
      " [17410]\n",
      " [   47]\n",
      " [    7]\n",
      " [  734]\n",
      " [ 1539]\n",
      " [21020]\n",
      " [35775]\n",
      " [  103]\n",
      " [   47]\n",
      " [    7]\n",
      " [ 1539]\n",
      " [21020]\n",
      " [35775]\n",
      " [  103]\n",
      " [17410]\n",
      " [    7]\n",
      " [35775]\n",
      " [  103]\n",
      " [17410]\n",
      " [   47]\n",
      " [  265]\n",
      " [ 2019]\n",
      " [ 8901]\n",
      " [ 1797]\n",
      " [ 9979]\n",
      " [ 5552]\n",
      " [ 2019]\n",
      " [ 8901]\n",
      " [ 1797]\n",
      " [ 9979]\n",
      " [ 5552]\n",
      " [  265]\n",
      " [ 8901]\n",
      " [ 1797]\n",
      " [ 9979]\n",
      " [ 5552]\n",
      " [  265]\n",
      " [ 2019]\n",
      " [ 1797]\n",
      " [ 9979]\n",
      " [ 5552]\n",
      " [  265]]\n",
      "[[17748], [17748], [17748], [17748], [17748], [134, 58], [134, 58], [134, 58], [134, 58], [134, 58], [134, 58], [264, 4], [264, 4], [264, 4], [264, 4], [264, 4], [264, 4], [264, 4], [4256, 0], [4256, 0], [4256, 0], [4256, 0], [4256, 0], [4256, 0], [4256, 0], [4256, 0], [4140, 2], [4140, 2], [4140, 2], [4140, 2], [4140, 2], [4140, 2], [4140, 2], [4140, 2], [3159], [3159], [3159], [3159], [3159], [3159], [3159], [3159], [14563], [14563], [14563], [14563], [14563], [14563], [14563], [9341], [9341], [9341], [9341], [9341], [9341], [10055, 3], [10055, 3], [10055, 3], [10055, 3], [10055, 3], [6114, 43], [6114, 43], [6114, 43], [6114, 43], [6114, 43], [134, 58], [134, 58], [134, 58], [134, 58], [134, 58], [134, 58], [264, 4], [264, 4], [264, 4], [264, 4], [264, 4], [264, 4], [264, 4], [14559], [14559], [14559], [14559], [14559], [14559], [14559], [14559], [4140, 2], [4140, 2], [4140, 2], [4140, 2], [4140, 2], [4140, 2], [4140, 2], [4140, 2], [5347, 535], [5347, 535], [5347, 535], [5347, 535], [5347, 535], [5347, 535], [5347, 535], [5347, 535], [5347, 535], [2711, 60], [2711, 60], [2711, 60], [2711, 60], [2711, 60], [2711, 60], [2711, 60], [2711, 60], [1203, 13], [1203, 13], [1203, 13], [1203, 13], [1203, 13], [1203, 13], [1203, 13], [9334], [9334], [9334], [9334], [9334], [9334], [56, 3], [56, 3], [56, 3], [56, 3], [7215], [7215], [7215], [7215], [7215], [815], [815], [815], [815], [815], [165, 14617, 17], [165, 14617, 17], [165, 14617, 17], [165, 14617, 17], [165, 14617, 17], [9944], [9944], [9944], [9944], [9944], [2717, 61], [2717, 61]]\n"
     ]
    }
   ],
   "source": [
    "print(batch_size)\n",
    "print(input_li_size)\n",
    "def generate_batch(iter, batch_size, input_li, output_li):\n",
    "    index = (iter % (input_li_size//batch_size)) * batch_size\n",
    "    batch_input = input_li[index:index+batch_size]\n",
    "    batch_output_li = output_li[index:index+batch_size]\n",
    "    batch_output = [[i] for i in batch_output_li]\n",
    "\n",
    "    return np.array(batch_input), np.array(batch_output)\n",
    "\n",
    "batch_inputs, batch_labels = generate_batch(0, batch_size, input_li, output_li)\n",
    "print(np.shape(batch_inputs))\n",
    "print(batch_inputs)\n",
    "print(np.shape(batch_labels))\n",
    "print(batch_labels)\n",
    "word_list = []\n",
    "for word in batch_inputs:\n",
    "    word_list.append(word_to_pos_dict[word])\n",
    "print(word_list)\n",
    "#     for pos in word_to_pos_dict[word]:\n",
    "#         print(pos)\n",
    "#         print(pos_reverse_dict[pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-859141a89a0d>:49: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 150\n",
    "num_sampled = 50\n",
    "learning_rate = 1.0\n",
    "\n",
    "valid_size = 20     # Random set of words to evaluate similarity on.\n",
    "valid_window = 200  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False) # 200까지 숫자 중에서 랜덤하게 20개 뽑음\n",
    "\n",
    "# tensorflow 신경망 모델 그래프 생성\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    words_matrix = [tf.placeholder(tf.int32, shape=None) for _ in range(batch_size)] # batch_size만큼의 word를 형태소로\n",
    "    vocabulary_matrix = [tf.placeholder(tf.int32, shape=None) for _ in range(vocabulary_size)] # word_dict만큼의 word를 형태소로.. 인거 같은데 안씀\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    # \"/device:GPU:0\"\n",
    "    with tf.device('/cpu:0'):\n",
    "        # embedding vector -> 우리가 원하는 최종 출력\n",
    "        pos_embeddings = tf.Variable(tf.random_uniform([pos_size, embedding_size], -1.0, 1.0), name='pos_embeddings')\n",
    "\n",
    "        word_vec_list = []\n",
    "        for i in range(batch_size):\n",
    "            word_vec = tf.reduce_sum(tf.nn.embedding_lookup(pos_embeddings, words_matrix[i]), 0)\n",
    "            word_vec_list.append(word_vec)\n",
    "        word_embeddings = tf.stack(word_vec_list) # word의 각 형태소를 embedding한 vector\n",
    "    \n",
    "        # Noise-Contrastive Estimation\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)), name='nce_weights'\n",
    "        )\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]), name='nce_biases')\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights,\n",
    "                       biases=nce_biases,\n",
    "                       labels=train_labels,\n",
    "                       inputs=word_embeddings,\n",
    "                       num_sampled=num_sampled,\n",
    "                       num_classes=vocabulary_size))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Compute the cosine similarity between minibatch exaples and all embeddings.\n",
    "    # 임의의 word로 유사도 검증\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(pos_embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = pos_embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of iterations for each epoch : 147721\n",
      "Initialized - Tensorflow\n",
      "Average loss at step  0 :  227.52951049804688\n",
      "Nearest to ('가', 'Eomi'): ('Golden', 'Alpha'), ('APRIL', 'Alpha'), ('고정식', 'Noun'), ('amentes', 'Alpha'), ('곡괭이', 'Noun'), ('Wake', 'Alpha'), ('앰비션', 'Noun'), ('조세', 'Noun'),\n",
      "Nearest to ('팀', 'Noun'): ('Targeting', 'Alpha'), ('쿠소', 'Noun'), ('이대진', 'Noun'), ('야유', 'Noun'), ('왼편', 'Noun'), ('사각형', 'Noun'), ('거부권', 'Noun'), ('Operation', 'Alpha'),\n",
      "Nearest to ('세', 'Noun'): ('낸시', 'Noun'), ('구분', 'Noun'), ('탱커', 'Noun'), ('아육대', 'Noun'), ('PURGATORY', 'Alpha'), ('속삭이는', 'Verb'), ('변화', 'Noun'), ('시작해서', 'Verb'),\n",
      "Nearest to ('야', 'Eomi'): ('쿵푸팬더', 'Noun'), ('딜러', 'Noun'), ('추진하겠', 'Verb'), ('릴', 'Noun'), ('변화하지', 'Verb'), ('개발하기', 'Verb'), ('couldn', 'Alpha'), ('구로', 'Noun'),\n",
      "Nearest to ('8', 'Number'): ('두드러지는', 'Adjective'), ('날카로운', 'Adjective'), ('바디', 'Noun'), ('삼각함수', 'Noun'), ('변치', 'Noun'), ('늘', 'Verb'), ('Last', 'Alpha'), ('특촬', 'Noun'),\n",
      "Nearest to ('권', 'Suffix'): ('익명', 'Noun'), ('y', 'Alpha'), ('진통제', 'Noun'), ('해안', 'Noun'), ('소유권', 'Noun'), ('EVER', 'Alpha'), ('윌프레드', 'Noun'), ('태조', 'Noun'),\n",
      "Nearest to ('아', 'Exclamation'): ('iDOLM', 'Alpha'), ('성숙하고', 'Verb'), ('사천왕', 'Noun'), ('실수', 'Noun'), ('뮤직', 'Noun'), ('닉스', 'Noun'), ('런타임', 'Noun'), ('뼈대', 'Noun'),\n",
      "Nearest to ('라고', 'Josa'): ('레르가스', 'Noun'), ('Pledger', 'Alpha'), ('월화드라마', 'Noun'), ('반송파', 'Noun'), ('돌아다녔', 'Verb'), ('코넬', 'Noun'), ('에이수스', 'Noun'), ('미하', 'Noun'),\n",
      "Nearest to ('이라고', 'Josa'): ('얽히', 'Noun'), ('레나', 'Noun'), ('한일전', 'Noun'), ('기', 'Verb'), ('Active', 'Alpha'), ('소시지', 'Noun'), ('멍', 'Noun'), ('관대', 'Noun'),\n",
      "Nearest to ('15', 'Number'): ('Rebel', 'Alpha'), ('인지', 'Eomi'), ('낮더', 'Adjective'), ('발음', 'Noun'), ('진과', 'Noun'), ('제공했', 'Verb'), ('황해도', 'Noun'), ('존경하고', 'Verb'),\n",
      "Nearest to ('하며', 'Verb'): ('02141', 'Number'), ('유조선', 'Noun'), ('바다로', 'Noun'), ('학계', 'Noun'), ('행해졌', 'Verb'), ('형국', 'Noun'), ('따라온', 'Verb'), ('산타나', 'Noun'),\n",
      "Nearest to ('지', 'Eomi'): ('뉴욕', 'Noun'), ('행정부', 'Noun'), ('빈칸', 'Noun'), ('주원', 'Noun'), ('치기', 'Noun'), ('선택', 'Noun'), ('김주성', 'Noun'), ('멋있는', 'Adjective'),\n",
      "Nearest to ('게', 'Eomi'): ('조정할', 'Verb'), ('eat', 'Alpha'), ('타케베', 'Noun'), ('AFTER', 'Alpha'), ('당혹', 'Noun'), ('little', 'Alpha'), ('두드러진', 'Adjective'), ('내세', 'Verb'),\n",
      "Nearest to ('가', 'Verb'): ('줌샷', 'Noun'), ('자폭', 'Noun'), ('처사', 'Noun'), ('어리석', 'Adjective'), ('발급', 'Noun'), ('천금', 'Noun'), ('곳일', 'Noun'), ('taking', 'Alpha'),\n",
      "Nearest to ('던', 'Eomi'): ('오픈', 'Noun'), ('이도', 'Noun'), ('유지하기', 'Verb'), ('Coin', 'Alpha'), ('습성', 'Noun'), ('일정량', 'Noun'), ('블로거', 'Noun'), ('조절하면', 'Verb'),\n",
      "Nearest to ('를', 'Noun'): ('총알', 'Noun'), ('팬암', 'Noun'), ('마쥬', 'Noun'), ('자유로이', 'Adverb'), ('골인', 'Noun'), ('Frame', 'Alpha'), ('깍', 'Adverb'), ('티저', 'Noun'),\n",
      "Nearest to ('일', 'Noun'): ('Hz', 'Alpha'), ('박재범', 'Noun'), ('게', 'Noun'), ('뺏고', 'Verb'), ('라임', 'Noun'), ('브래스', 'Noun'), ('받았', 'Verb'), ('평가했', 'Verb'),\n",
      "Nearest to ('레이', 'Noun'): ('장웅', 'Noun'), ('사신', 'Noun'), ('유려', 'Noun'), ('ESP', 'Alpha'), ('clear', 'Alpha'), ('흘러가는', 'Verb'), ('일류', 'Noun'), ('입막음', 'Noun'),\n",
      "Nearest to ('도', 'Eomi'): ('FUNKY', 'Alpha'), ('김도현', 'Noun'), ('Shy', 'Alpha'), ('수납', 'Noun'), ('Tear', 'Alpha'), ('Come', 'Alpha'), ('뺨', 'Noun'), ('히피', 'Noun'),\n",
      "Nearest to ('까지', 'Josa'): ('중', 'Noun'), ('케빈', 'Noun'), ('공연', 'Noun'), ('왼손', 'Noun'), ('Bo', 'Alpha'), ('SW', 'Alpha'), ('마이크로', 'Noun'), ('Yoshitaka', 'Alpha'),\n",
      "Average loss at step  44316 :  615.4417446576953\n",
      "Average loss at step  88632 :  164.5354142833557\n",
      "Nearest to ('가', 'Eomi'): ('APRIL', 'Alpha'), ('Motif', 'Alpha'), ('Wake', 'Alpha'), ('웃통', 'Noun'), ('곡괭이', 'Noun'), ('고정식', 'Noun'), ('대주자', 'Noun'), ('도돈', 'Noun'),\n",
      "Nearest to ('팀', 'Noun'): ('모두', 'Noun'), ('그리고', 'Conjunction'), ('멤버', 'Noun'), ('산성', 'Noun'), ('dung', 'Alpha'), ('그', 'Noun'), ('클럼프', 'Noun'), ('이후', 'Noun'),\n",
      "Nearest to ('세', 'Noun'): ('두', 'Noun'), ('dung', 'Alpha'), ('하지만', 'Conjunction'), ('RESET', 'Alpha'), ('연금술사', 'Noun'), ('되는', 'Verb'), ('띵', 'Noun'), ('한', 'Verb'),\n",
      "Nearest to ('야', 'Eomi'): ('구로', 'Noun'), ('지향성', 'Noun'), ('깎으', 'Verb'), ('변화하지', 'Verb'), ('직선제', 'Noun'), ('다고', 'Eomi'), ('Sally', 'Alpha'), ('란드', 'Noun'),\n",
      "Nearest to ('8', 'Number'): ('6', 'Number'), ('11', 'Number'), ('5', 'Number'), ('9', 'Number'), ('12', 'Number'), ('10', 'Number'), ('7', 'Number'), ('20', 'Number'),\n",
      "Nearest to ('권', 'Suffix'): ('y', 'Alpha'), ('않기', 'Verb'), ('익명', 'Noun'), ('탈산', 'Noun'), ('하지원', 'Noun'), ('IAAF', 'Alpha'), ('승부차기', 'Noun'), ('소유권', 'Noun'),\n",
      "Nearest to ('아', 'Exclamation'): ('같은', 'Adjective'), ('클럼프', 'Noun'), ('그러나', 'Conjunction'), ('dung', 'Alpha'), ('DotA', 'Alpha'), ('zip', 'Alpha'), ('바로', 'Noun'), ('또한', 'Noun'),\n",
      "Nearest to ('라고', 'Josa'): ('사람', 'Noun'), ('라는', 'Josa'), ('하지만', 'Conjunction'), ('한', 'Verb'), ('자신', 'Noun'), ('하고', 'Verb'), ('이런', 'Adjective'), ('때', 'Noun'),\n",
      "Nearest to ('이라고', 'Josa'): ('라고', 'Josa'), ('사람', 'Noun'), ('하고', 'Verb'), ('잘', 'Verb'), ('보면', 'Verb'), ('내', 'Noun'), ('자신', 'Noun'), ('하지만', 'Conjunction'),\n",
      "Nearest to ('15', 'Number'): ('9', 'Number'), ('10', 'Number'), ('12', 'Number'), ('6', 'Number'), ('8', 'Number'), ('11', 'Number'), ('5', 'Number'), ('7', 'Number'),\n",
      "Nearest to ('하며', 'Verb'): ('그', 'Noun'), ('하지만', 'Conjunction'), ('zip', 'Alpha'), ('클럼프', 'Noun'), ('그리고', 'Conjunction'), ('의', 'Josa'), ('때', 'Noun'), ('결국', 'Adverb'),\n",
      "Nearest to ('지', 'Eomi'): ('하지', 'Verb'), ('않았', 'Verb'), ('으나', 'Eomi'), ('전혀', 'Noun'), ('않으', 'Verb'), ('과학기술', 'Noun'), ('받지', 'Verb'), ('않는', 'Verb'),\n",
      "Nearest to ('게', 'Eomi'): ('안', 'Noun'), ('잘', 'Verb'), ('해', 'Noun'), ('도달할', 'Verb'), ('상황', 'Noun'), ('팰렁스', 'Noun'), ('위험', 'Noun'), ('하게', 'Verb'),\n",
      "Nearest to ('가', 'Verb'): ('zip', 'Alpha'), ('되는', 'Verb'), ('dung', 'Alpha'), ('열역학', 'Noun'), ('또한', 'Noun'), ('DotA', 'Alpha'), ('그리고', 'Conjunction'), ('같은', 'Adjective'),\n",
      "Nearest to ('던', 'Eomi'): ('이라는', 'Josa'), ('많은', 'Adjective'), ('새미', 'Noun'), ('날', 'Noun'), ('닥터', 'Noun'), ('하지만', 'Conjunction'), ('했', 'Verb'), ('마지막', 'Noun'),\n",
      "Nearest to ('를', 'Noun'): ('zip', 'Alpha'), ('dung', 'Alpha'), ('DotA', 'Alpha'), ('아틀리에', 'Noun'), ('띵', 'Noun'), ('그', 'Noun'), ('점성술', 'Noun'), ('그리고', 'Conjunction'),\n",
      "Nearest to ('일', 'Noun'): ('년', 'Noun'), ('이후', 'Noun'), ('후', 'Noun'), ('그리고', 'Conjunction'), ('아틀리에', 'Noun'), ('전', 'Noun'), ('현재', 'Noun'), ('주', 'Noun'),\n",
      "Nearest to ('레이', 'Noun'): ('역시', 'Noun'), ('되었', 'Verb'), ('장웅', 'Noun'), ('펜타비전', 'Noun'), ('띵', 'Noun'), ('기존', 'Noun'), ('통해', 'Noun'), ('Stade', 'Alpha'),\n",
      "Nearest to ('도', 'Eomi'): ('뺨', 'Noun'), ('문화원', 'Noun'), ('기도', 'Noun'), ('내야안타', 'Noun'), ('페더', 'Noun'), ('블럭', 'Noun'), ('반장', 'Noun'), ('SILVER', 'Alpha'),\n",
      "Nearest to ('까지', 'Josa'): ('Try', 'Alpha'), ('비유', 'Noun'), ('장수원', 'Noun'), ('다림질', 'Noun'), ('비정상회담', 'Noun'), ('Mystic', 'Alpha'), ('인식할', 'Verb'), ('놓쳐', 'Verb'),\n",
      "Average loss at step  132948 :  133.9222630942017\n",
      "Average loss at step  177264 :  123.40393812322617\n",
      "Average loss at step  221580 :  124.10671935909615\n",
      "Nearest to ('가', 'Eomi'): ('으아', 'Exclamation'), ('zip', 'Alpha'), ('Quantus', 'Alpha'), ('DotA', 'Alpha'), ('연금술사', 'Noun'), ('결국', 'Adverb'), ('뒤', 'Noun'), ('후', 'Noun'),\n",
      "Nearest to ('팀', 'Noun'): ('모두', 'Noun'), ('예선', 'Noun'), ('멤버', 'Noun'), ('산성', 'Noun'), ('하나', 'Noun'), ('총', 'Noun'), ('중', 'Noun'), ('각', 'Noun'),\n",
      "Nearest to ('세', 'Noun'): ('두', 'Noun'), ('하나', 'Noun'), ('그', 'Noun'), ('플레이어', 'Noun'), ('dung', 'Alpha'), ('번째', 'Suffix'), ('한', 'Verb'), ('또', 'Noun'),\n",
      "Nearest to ('야', 'Eomi'): ('다고', 'Eomi'), ('구로', 'Noun'), ('란드', 'Noun'), ('리투쿠즈웨', 'Noun'), ('마다', 'Josa'), ('카를', 'Noun'), ('추진하겠', 'Verb'), ('잡겠', 'Verb'),\n",
      "Nearest to ('8', 'Number'): ('7', 'Number'), ('9', 'Number'), ('11', 'Number'), ('5', 'Number'), ('12', 'Number'), ('10', 'Number'), ('13', 'Number'), ('6', 'Number'),\n",
      "Nearest to ('권', 'Suffix'): ('탈산', 'Noun'), ('빈지노', 'Noun'), ('y', 'Alpha'), ('해치', 'Verb'), ('IAAF', 'Alpha'), ('승마', 'Noun'), ('정공', 'Noun'), ('급진', 'Noun'),\n",
      "Nearest to ('아', 'Exclamation'): ('클럼프', 'Noun'), ('dung', 'Alpha'), ('WRONG', 'Alpha'), ('WM', 'Alpha'), ('단', 'Noun'), ('물론', 'Noun'), ('Silence', 'Alpha'), ('다른', 'Noun'),\n",
      "Nearest to ('라고', 'Josa'): ('말', 'Noun'), ('이라고', 'Josa'), ('사람', 'Noun'), ('라는', 'Josa'), ('걸', 'Noun'), ('하고', 'Verb'), ('고', 'Noun'), ('하지만', 'Conjunction'),\n",
      "Nearest to ('이라고', 'Josa'): ('라고', 'Josa'), ('하고', 'Verb'), ('라는', 'Josa'), ('사람', 'Noun'), ('말', 'Noun'), ('것', 'Noun'), ('은', 'Noun'), ('시에스타', 'Noun'),\n",
      "Nearest to ('15', 'Number'): ('10', 'Number'), ('11', 'Number'), ('17', 'Number'), ('9', 'Number'), ('6', 'Number'), ('30', 'Number'), ('12', 'Number'), ('25', 'Number'),\n",
      "Nearest to ('하며', 'Verb'): ('잔돈', 'Noun'), ('클럼프', 'Noun'), ('결국', 'Adverb'), ('zip', 'Alpha'), ('시에스타', 'Noun'), ('DotA', 'Alpha'), ('으아', 'Exclamation'), ('biscuit', 'Alpha'),\n",
      "Nearest to ('지', 'Eomi'): ('하지', 'Verb'), ('전혀', 'Noun'), ('받지', 'Verb'), ('나오지', 'Verb'), ('않아', 'Verb'), ('있지', 'Adjective'), ('맞지', 'Verb'), ('견습', 'Noun'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to ('게', 'Eomi'): ('안', 'Noun'), ('게', 'Josa'), ('어려워졌', 'Verb'), ('하게', 'Verb'), ('가는', 'Verb'), ('그런데', 'Conjunction'), ('만나게', 'Verb'), ('동면', 'Noun'),\n",
      "Nearest to ('가', 'Verb'): ('와', 'Noun'), ('dung', 'Alpha'), ('아닌', 'Adjective'), ('같은', 'Adjective'), ('즉', 'Noun'), ('나온', 'Verb'), ('nous', 'Alpha'), ('를', 'Noun'),\n",
      "Nearest to ('던', 'Eomi'): ('드러났', 'Verb'), ('중증', 'Noun'), ('한의사', 'Noun'), ('remywiki', 'Alpha'), ('회복했', 'Verb'), ('drive', 'Alpha'), ('성벽', 'Noun'), ('박형규', 'Noun'),\n",
      "Nearest to ('를', 'Noun'): ('biscuit', 'Alpha'), ('zip', 'Alpha'), ('dung', 'Alpha'), ('으아', 'Exclamation'), ('그', 'Noun'), ('와', 'Noun'), ('때', 'Noun'), ('하면', 'Verb'),\n",
      "Nearest to ('일', 'Noun'): ('월', 'Noun'), ('년', 'Noun'), ('날', 'Noun'), ('열린', 'Verb'), ('이후', 'Noun'), ('망하고', 'Verb'), ('동안', 'Noun'), ('공연', 'Noun'),\n",
      "Nearest to ('레이', 'Noun'): ('통해', 'Noun'), ('역시', 'Noun'), ('클럼프', 'Noun'), ('zip', 'Alpha'), ('Stade', 'Alpha'), ('띵', 'Noun'), ('시에스타', 'Noun'), ('함께', 'Adverb'),\n",
      "Nearest to ('도', 'Eomi'): ('기도', 'Noun'), ('아타리', 'Noun'), ('병역법', 'Noun'), ('티니', 'Noun'), ('AOA', 'Alpha'), ('라우터', 'Noun'), ('Bee', 'Alpha'), ('벌꿀', 'Noun'),\n",
      "Nearest to ('까지', 'Josa'): ('상계동', 'Noun'), ('금물', 'Noun'), ('필요한', 'Adjective'), ('총리대신', 'Noun'), ('들어간', 'Verb'), ('Battlecruiser', 'Alpha'), ('u', 'Alpha'), ('심금', 'Noun'),\n",
      "Average loss at step  265896 :  118.76081635947526\n",
      "Average loss at step  310212 :  114.83686689312756\n",
      "Nearest to ('가', 'Eomi'): ('는데', 'Eomi'), ('서', 'Josa'), ('고', 'Eomi'), ('러', 'Eomi'), ('메커니즘', 'Noun'), ('나가면', 'Verb'), ('데', 'Eomi'), ('죽어', 'Verb'),\n",
      "Nearest to ('팀', 'Noun'): ('각', 'Noun'), ('나머지', 'Noun'), ('모두', 'Noun'), ('예선', 'Noun'), ('중', 'Noun'), ('총', 'Noun'), ('본선', 'Noun'), ('각각', 'Noun'),\n",
      "Nearest to ('세', 'Noun'): ('두', 'Noun'), ('한', 'Verb'), ('사람', 'Noun'), ('몇', 'Noun'), ('플레이어', 'Noun'), ('하나', 'Noun'), ('으아', 'Exclamation'), ('자신', 'Noun'),\n",
      "Nearest to ('야', 'Eomi'): ('다고', 'Eomi'), ('해야', 'Verb'), ('고', 'Noun'), ('도', 'Eomi'), ('최대한', 'Noun'), ('면', 'Eomi'), ('이라고', 'Josa'), ('로코', 'Noun'),\n",
      "Nearest to ('8', 'Number'): ('11', 'Number'), ('7', 'Number'), ('12', 'Number'), ('9', 'Number'), ('13', 'Number'), ('10', 'Number'), ('15', 'Number'), ('3', 'Number'),\n",
      "Nearest to ('권', 'Suffix'): ('해치', 'Verb'), ('빈지노', 'Noun'), ('급진', 'Noun'), ('펠레', 'Noun'), ('JYP', 'Alpha'), ('상병', 'Noun'), ('IAAF', 'Alpha'), ('y', 'Alpha'),\n",
      "Nearest to ('아', 'Exclamation'): ('전혀', 'Noun'), ('아직', 'Noun'), ('WRONG', 'Alpha'), ('안', 'Noun'), ('심지어', 'Noun'), ('채', 'Noun'), ('WM', 'Alpha'), ('없었', 'Adjective'),\n",
      "Nearest to ('라고', 'Josa'): ('이라고', 'Josa'), ('고', 'Noun'), ('우리', 'Noun'), ('라는', 'Josa'), ('거', 'Noun'), ('걸', 'Noun'), ('그냥', 'Noun'), ('그런', 'Adjective'),\n",
      "Nearest to ('이라고', 'Josa'): ('라고', 'Josa'), ('고', 'Noun'), ('즉', 'Noun'), ('라는', 'Josa'), ('그런데', 'Conjunction'), ('실제', 'Noun'), ('지금', 'Noun'), ('이라는', 'Josa'),\n",
      "Nearest to ('15', 'Number'): ('13', 'Number'), ('22', 'Number'), ('25', 'Number'), ('14', 'Number'), ('7', 'Number'), ('10', 'Number'), ('11', 'Number'), ('12', 'Number'),\n",
      "Nearest to ('하며', 'Verb'): ('그리고', 'Conjunction'), ('하지만', 'Conjunction'), ('클럼프', 'Noun'), ('하게', 'Verb'), ('biscuit', 'Alpha'), ('했', 'Verb'), ('시에스타', 'Noun'), ('아틀리에', 'Noun'),\n",
      "Nearest to ('지', 'Eomi'): ('하지', 'Verb'), ('지', 'Josa'), ('나오지', 'Verb'), ('받지', 'Verb'), ('전혀', 'Noun'), ('보이지', 'Noun'), ('그다지', 'Noun'), ('있지', 'Adjective'),\n",
      "Nearest to ('게', 'Eomi'): ('게', 'Josa'), ('울리는', 'Verb'), ('으나', 'Eomi'), ('물린', 'Noun'), ('분리한', 'Verb'), ('우린', 'Noun'), ('받게', 'Verb'), ('연합뉴스', 'Noun'),\n",
      "Nearest to ('가', 'Verb'): ('와', 'Noun'), ('nous', 'Alpha'), ('를', 'Noun'), ('원래', 'Noun'), ('이', 'Noun'), ('에서', 'Noun'), ('는', 'Verb'), ('보통', 'Noun'),\n",
      "Nearest to ('던', 'Eomi'): ('오티즈', 'Noun'), ('앤드류', 'Noun'), ('없어지는', 'Verb'), ('어썰트', 'Noun'), ('발견하는', 'Verb'), ('겪게', 'Verb'), ('RFB', 'Alpha'), ('부렸', 'Noun'),\n",
      "Nearest to ('를', 'Noun'): ('11', 'Number'), ('zip', 'Alpha'), ('biscuit', 'Alpha'), ('와', 'Noun'), ('15', 'Number'), ('2', 'Number'), ('mathrm', 'Alpha'), ('과', 'Noun'),\n",
      "Nearest to ('일', 'Noun'): ('통해', 'Noun'), ('이후', 'Noun'), ('점성술', 'Noun'), ('또한', 'Noun'), ('삼육구', 'Noun'), ('dung', 'Alpha'), ('진행되', 'Verb'), ('시에스타', 'Noun'),\n",
      "Nearest to ('레이', 'Noun'): ('미국', 'Noun'), ('존', 'Noun'), ('클럼프', 'Noun'), ('Waiting', 'Alpha'), ('zip', 'Alpha'), ('영국', 'Noun'), ('Stade', 'Alpha'), ('무료', 'Noun'),\n",
      "Nearest to ('도', 'Eomi'): ('기도', 'Noun'), ('다고', 'Eomi'), ('야', 'Eomi'), ('안타깝', 'Adjective'), ('흡사', 'Noun'), ('병역법', 'Noun'), ('server', 'Alpha'), ('위팀', 'Noun'),\n",
      "Nearest to ('까지', 'Josa'): ('부터', 'Josa'), ('부총리', 'Noun'), ('개', 'Noun'), ('LOVERS', 'Alpha'), ('Computer', 'Alpha'), ('노력하는', 'Verb'), ('착하거', 'Adjective'), ('신원', 'Noun'),\n",
      "Average loss at step  354528 :  115.94639300287142\n",
      "Average loss at step  398844 :  114.99684605581314\n",
      "Average loss at step  443160 :  111.98270947585254\n"
     ]
    }
   ],
   "source": [
    "num_iterations = input_li_size // batch_size\n",
    "print(\"number of iterations for each epoch :\", num_iterations)\n",
    "epochs = 3\n",
    "num_steps = num_iterations * epochs + 1\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    init.run()\n",
    "    print(\"Initialized - Tensorflow\")\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(step, batch_size, input_li, output_li)\n",
    "\n",
    "        word_list = []\n",
    "        for word in batch_inputs:\n",
    "            word_list.append(word_to_pos_dict[word])\n",
    "\n",
    "        feed_dict = {}\n",
    "        for i in range(batch_size):\n",
    "            feed_dict[words_matrix[i]] = word_list[i]\n",
    "        feed_dict[train_inputs] = batch_inputs\n",
    "        feed_dict[train_labels] = batch_labels\n",
    "\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % (num_steps//10) == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            print(\"Average loss at step \", step, \": \", average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "        if step % (num_steps//4) == 0:\n",
    "            pos_embed = pos_embeddings.eval()\n",
    "\n",
    "            # Print nearest words\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_pos = pos_reverse_dict[valid_examples[i]]\n",
    "                top_k = 8\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % str(valid_pos)\n",
    "                for k in range(top_k):\n",
    "                    close_word = pos_reverse_dict[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, str(close_word))\n",
    "                print(log_str)\n",
    "\n",
    "    pos_embed = pos_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save vectors.\n",
    "def save_model(pos_list, embeddings, file_name):\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.write(str(len(pos_list)))\n",
    "        f.write(\" \")\n",
    "        f.write(str(embedding_size))\n",
    "        f.write(\"\\n\")\n",
    "        for i in range(len(pos_list)):\n",
    "            pos = pos_list[i]\n",
    "            f.write(str(pos).replace(\"', '\", \"','\") + \" \")\n",
    "            f.write(' '.join(map(str, embeddings[i])))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "# Save vectors\n",
    "save_model(pos_li, pos_embed, \"pos.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pk_story",
   "language": "python",
   "name": "pk_story"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
