{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "009_seqGAN.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg7emNDaAmgI"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt4L0Tq0ArkN"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Wk2ADzVBIl2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fde0a50a-a9d2-45f1-dfa0-dca7e2dbd046"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.2MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 11.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/a5/9781e2ef4ca92d09912c4794642c1653aea7607f473e156cf4d423a881a1/JPype1-1.2.1-cp37-cp37m-manylinux2010_x86_64.whl (457kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 57.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: beautifulsoup4, JPype1, colorama, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjsKLvmfBFrU"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import collections\n",
        "from konlpy.tag import Twitter, Kkma\n",
        "import pickle\n",
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import argparse\n",
        "\n",
        "import math\n",
        "import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w-_d987QWF1"
      },
      "source": [
        "from gensim.models.word2vec import Word2Vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bOAlx6PJWa6"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc5BHWiNfPnB"
      },
      "source": [
        "## Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otRogzSNfThA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doGs5VWZfrAA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHhLBRDYBUf9"
      },
      "source": [
        "## 1. load_embed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "K3omYPf5Azs_",
        "outputId": "fe376232-2c3a-495f-c663-d8761c49843e"
      },
      "source": [
        "def load_pretrained_vector(embed_path, load_path):\n",
        "    with open(embed_path, 'r') as inp, open(load_path, 'w') as outp:\n",
        "        pos_count = '30185'    # line count of the tsv file (as string)\n",
        "        dimensions = '200'    # vector size (as string)\n",
        "        outp.write(' '.join([pos_count, dimensions]) + '\\n')\n",
        "        for line in inp:\n",
        "            words = line.strip().split()\n",
        "            outp.write(' ')\n",
        "            if \"]\" in [w for word in words for w in word]:\n",
        "                line = line.strip().replace(\"]\", \"\")\n",
        "                words = line.strip().split()\n",
        "                outp.write(' '.join(words) + '\\n')\n",
        "            elif \"[\" in [w for word in words for w in word]:\n",
        "                line = line.strip().replace(\"[\", \"\")\n",
        "                words = line.strip().split()\n",
        "                outp.write(' '.join(words))\n",
        "            else:\n",
        "                outp.write(' '.join(words))\n",
        "\n",
        "def load_vec_file(filepath):\n",
        "    \"\"\"\n",
        "    Load .vec file. Get pos_dict, pos_embedding_vector\n",
        "    :param filepath: String, path of .vec file\n",
        "    :return:\n",
        "        pos_size, embedding_size, pos2idx, idx2pos, embedding_vec\n",
        "    \"\"\"\n",
        "    i = 0\n",
        "    with open(filepath, 'r') as fout:\n",
        "        pos_list = list()\n",
        "        embedding_list = list()\n",
        "        for line in fout:\n",
        "            line = line.strip()\n",
        "            if i == 0:\n",
        "                pos_size = int(line.split(\" \")[0])\n",
        "                embedding_size = int(line.split(\" \")[1])\n",
        "                i += 1\n",
        "                continue\n",
        "            vector_list = list()\n",
        "            line_sp = line.split(\" \")\n",
        "            for j in range(len(line_sp)):\n",
        "                if j == 0:\n",
        "                    continue\n",
        "                elif j == 1:\n",
        "                    # print(line_sp[j])\n",
        "                    pos_list.append(line_sp[j])\n",
        "                else:\n",
        "                    # print(line_sp[j])\n",
        "                    vector_list.append(line_sp[j])\n",
        "            embedding_list.append(vector_list)\n",
        "\n",
        "    pos2idx = dict()\n",
        "    for pos in pos_list:\n",
        "        pos2idx[pos] = len(pos2idx)\n",
        "    idx2pos = dict(zip(pos2idx.values(), pos2idx.keys()))\n",
        "    embedding_vec = np.array(embedding_list, dtype=np.float32)\n",
        "\n",
        "    return pos_size, embedding_size, pos2idx, idx2pos, embedding_vec\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    embed_path = \"./embed/ko.tsv\"\n",
        "    load_path = \"./embed/vec.txt\"\n",
        "\n",
        "    print(\"Loading {}...\".format(embed_path))\n",
        "    load_pretrained_vector(embed_path, load_path)\n",
        "    print(\"Saved Complete {} !! \".format(load_path))\n",
        "\n",
        "    # pos_size, embedding_size, pos2idx, idx2pos, embedding_vec = load_vec_file(load_path)\n",
        "    # print(pos_size)\n",
        "    # print(embedding_size)\n",
        "    # print(pos2idx)\n",
        "    # print(idx2pos)\n",
        "    # print(np.shape(embedding_vec))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading ./embed/ko.tsv...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-3ae7d3119b09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading {}...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mload_pretrained_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saved Complete {} !! \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-3ae7d3119b09>\u001b[0m in \u001b[0;36mload_pretrained_vector\u001b[0;34m(embed_path, load_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_pretrained_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mpos_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'30185'\u001b[0m    \u001b[0;31m# line count of the tsv file (as string)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mdimensions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'200'\u001b[0m    \u001b[0;31m# vector size (as string)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moutp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './embed/ko.tsv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK7NdKy7BVxE"
      },
      "source": [
        "## 2. preprocess_util"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7bY3b-SBBaw"
      },
      "source": [
        "np.random.seed(100)\n",
        "\n",
        "if not os.path.isdir('./data'):\n",
        "    os.mkdir('./data')\n",
        "\n",
        "def _save_pickle(path, data):\n",
        "    # save pkl\n",
        "    f = open(path, 'wb')\n",
        "    pickle.dump(data, f)\n",
        "    f.close()\n",
        "\n",
        "\n",
        "def sentence2pos(train_text, tag):\n",
        "    if tag == \"kkma\":\n",
        "        analyzer = Kkma()\n",
        "    elif tag == \"twitter\":\n",
        "        analyzer = Twitter()\n",
        "\n",
        "    sentences = list()\n",
        "    for line in train_text:\n",
        "        sentence = re.sub(r\"[^ㄱ-힣a-zA-Z0-9]+\", ' ', line)\n",
        "        if sentence:\n",
        "            sentence = analyzer.pos(sentence)\n",
        "            sentences.append(sentence)\n",
        "\n",
        "    pos_counter = [['UNK', -1]]  # 빈도수 문제로 word_dict에 없는 word를 처리하기 위함. unknown\n",
        "    pos_counter.extend(collections.Counter([word[0] for words in sentences for word in words]).most_common())\n",
        "    print(pos_counter)\n",
        "\n",
        "    pos_list = list()\n",
        "    for pos, _ in pos_counter:\n",
        "        pos_list.append(pos)\n",
        "\n",
        "    return sentences, pos_list\n",
        "\n",
        "\n",
        "def load_sentence(path):\n",
        "    sentences = list()\n",
        "    with open(path, 'r') as fout:\n",
        "        for line in fout:\n",
        "            pos_line = list()\n",
        "            for pos in line.split():\n",
        "                pos_line.append(pos)\n",
        "            sentences.append(pos_line)\n",
        "    return sentences\n",
        "\n",
        "\n",
        "def get_sentence_pos(input):\n",
        "    # Separate sentences into pos(morphemes)\n",
        "    # 문장을 형태소로 분리\n",
        "    # tag : \"kkma\" or \"twitter\"\n",
        "    print(\"Sentence to pos in progressing...\")\n",
        "    data, pos_list = sentence2pos(input, tag=\"twitter\")\n",
        "    print(\"poke_count: \", count)\n",
        "    print(len(data))\n",
        "    print(data[0])\n",
        "    print(\"pos_list: \", pos_list)\n",
        "\n",
        "    # save pkl\n",
        "    _save_pickle('./data/1_pk_real_data.pkl', data)\n",
        "    _save_pickle('./data/pk_pos_list.pkl', pos_list)\n",
        "\n",
        "    # save txt\n",
        "    f = open('./data/1_pk_real_data.txt', 'w')\n",
        "    for token in data:\n",
        "        for word in token:\n",
        "            word = str(word) + ' '\n",
        "            f.write(word)\n",
        "        f.write('\\n')\n",
        "    f.close()\n",
        "\n",
        "\n",
        "def get_preprocess_data(embedpath, data_pos_list):\n",
        "\n",
        "    # embed text 파일 읽기\n",
        "    print(\"Loading embedding vector...\")\n",
        "    i = 0\n",
        "    with open(embedpath, 'r') as fout:\n",
        "        embed_pos_list = list()\n",
        "        embedding_list = list()\n",
        "\n",
        "        for line in fout:\n",
        "            line = line.strip()\n",
        "            if i == 0:\n",
        "                pos_size = int(line.split(\" \")[0])\n",
        "                embedding_size = int(line.split(\" \")[1])\n",
        "                i += 1\n",
        "                continue\n",
        "            vector_list = list()\n",
        "            line_sp = line.split(\" \")\n",
        "            for j in range(len(line_sp)):\n",
        "                if j == 0:\n",
        "                    continue\n",
        "                elif j == 1:\n",
        "                    # print(line_sp[j])\n",
        "                    embed_pos_list.append(line_sp[j])\n",
        "                else:\n",
        "                    # print(line_sp[j])\n",
        "                    vector_list.append(line_sp[j])\n",
        "            embedding_list.append(vector_list)\n",
        "\n",
        "    # embed vector의  pos2idx, idx2pos, embedding_vec 만듬\n",
        "    pos2idx = dict()\n",
        "    for pos in embed_pos_list:\n",
        "        pos2idx[pos] = len(pos2idx)\n",
        "    idx2pos = dict(zip(pos2idx.values(), pos2idx.keys()))\n",
        "    print(pos2idx)\n",
        "    print(idx2pos)\n",
        "\n",
        "    embedding_vec = np.array(embedding_list, dtype=np.float32)\n",
        "    print(\"before embed: \", np.shape(embedding_vec))\n",
        "\n",
        "    print(\"Create new embedding vector...\")\n",
        "    # 현재 데이터에 해당되는 embedding vector만 추출.\n",
        "    exist_idx = list()\n",
        "    nonexist_pos = list()\n",
        "    for data in data_pos_list:\n",
        "        if data in list(pos2idx.keys()):\n",
        "            exist_idx.append(pos2idx[data])\n",
        "        else:\n",
        "            nonexist_pos.append(data)\n",
        "\n",
        "    embedding_vec = embedding_vec[sorted(exist_idx)]\n",
        "    print(sorted(exist_idx))\n",
        "    print(\"after embed: \", np.shape(embedding_vec))\n",
        "\n",
        "    # 현재 데이터에는 있지만 embedding vector에 없는 데이터는 무작위 vector로 embedding vector에 추가\n",
        "    start_embed = np.random.randn(1, embedding_size)\n",
        "    add_embed = np.random.randn(len(nonexist_pos), embedding_size)\n",
        "    embedding_vec = np.concatenate([start_embed, embedding_vec, add_embed], axis=0)\n",
        "    print(len(nonexist_pos))\n",
        "    print(\"after embed2: \", np.shape(embedding_vec))\n",
        "\n",
        "    # 현재 데이터와 새로 만들어진 embedding vector에 맞는 pos2idx, idx2pos 만듬\n",
        "    pos2idx = dict()\n",
        "    pos2idx[\"<start>\"] = len(pos2idx)\n",
        "    for idx in sorted(exist_idx):\n",
        "        pos = idx2pos[idx]\n",
        "        pos2idx[pos] = len(pos2idx)\n",
        "    for pos in nonexist_pos:\n",
        "        pos2idx[pos] = len(pos2idx)\n",
        "    idx2pos = dict(zip(pos2idx.values(), pos2idx.keys()))\n",
        "\n",
        "    pos_size = len(pos2idx)\n",
        "\n",
        "    # save pkl\n",
        "    _save_pickle('./data/pk_pos2idx.pkl', pos2idx)\n",
        "    _save_pickle('./data/pk_idx2pos.pkl', idx2pos)\n",
        "    _save_pickle('./data/pretrain_embedding_vec.pkl', embedding_vec)\n",
        "    print(\"Save all data as pkl !!\")\n",
        "\n",
        "    return pos_size, embedding_size\n",
        "\n",
        "\n",
        "def _pkl_loading_test():\n",
        "    # load sentences separated by pos (pkl)\n",
        "    a = open('./data/1_pk_real_data.pkl', 'rb')\n",
        "    sents = pickle.load(a)\n",
        "\n",
        "    # load pos_list (pkl)\n",
        "    a = open('./data/pk_pos_list.pkl', 'rb')\n",
        "    pos_list = pickle.load(a)\n",
        "\n",
        "    # load pos2idx (pkl)\n",
        "    a = open('./data/pk_pos2idx.pkl', 'rb')\n",
        "    pos2idx = pickle.load(a)\n",
        "\n",
        "    # load idx2pos (pkl)\n",
        "    a = open('./data/pk_idx2pos.pkl', 'rb')\n",
        "    idx2pos = pickle.load(a)\n",
        "\n",
        "    # load embedding_vec (pkl)\n",
        "    a = open('./data/pretrain_embedding_vec.pkl', 'rb')\n",
        "    embedding_vec = pickle.load(a)\n",
        "\n",
        "    print(sents)\n",
        "    print(len(pos_list))\n",
        "    print(pos2idx)\n",
        "    print(idx2pos)\n",
        "    print(np.shape(embedding_vec))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    DATA_PATH = \"./data/\"\n",
        "    embed_path = \"./embed/vec.txt\"\n",
        "\n",
        "    data = pd.read_csv(DATA_PATH + 'Crawling_Lyrics.csv')\n",
        "    data.dropna(axis=0, inplace = True)\n",
        "    song_lyrics = list(data['lyrics'])\n",
        "\n",
        "    lyrics = []\n",
        "    for song_lyric in song_lyrics:\n",
        "      sentences = []\n",
        "      for sent in song_lyric.split('\\n'):\n",
        "        if sent == '':\n",
        "          continue\n",
        "        sent = re.sub(r\"[^ㄱ-힣a-zA-Z0-9]+\", ' ', sent)\n",
        "        if sent:\n",
        "          sent = analyzer.pos(sent)\n",
        "        sentences.append(sent)\n",
        "      lyrics.append(sentences)\n",
        "    print(lyrics)\n",
        "\n",
        "    # sentence와 pos_list pkl 만듬\n",
        "    # 이미 pkl 만들었으면 주석 처리, 처음 사용시 주석 해제\n",
        "    print(\"Data Loading and indexing...\")\n",
        "    get_sentence_pos(input)\n",
        "\n",
        "    # load sentences separated by pos (pkl)\n",
        "    a = open('./data/1_pk_real_data.pkl', 'rb')\n",
        "    sents = pickle.load(a)\n",
        "\n",
        "    # load pos_list (pkl)\n",
        "    a = open('./data/pk_pos_list.pkl', 'rb')\n",
        "    pos_list = pickle.load(a)\n",
        "    print(pos_list)\n",
        "\n",
        "    # make embedding vector and etc\n",
        "    print(\"Data preprocessing in progress..\")\n",
        "    pos_size, embedding_size = get_preprocess_data(embed_path, pos_list)\n",
        "    print(\"pos_size: \", pos_size)\n",
        "    print(\"embedding_size: \", embedding_size)\n",
        "\n",
        "    print(\"#### test ####\")\n",
        "    _pkl_loading_test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nlNi_iHb2cj"
      },
      "source": [
        "## 3. preprocess_data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1hmyM2lBWwD"
      },
      "source": [
        "def _save_pickle(path, data):\n",
        "    # save pkl\n",
        "    f = open(path, 'wb')\n",
        "    pickle.dump(data, f)\n",
        "    f.close()\n",
        "\n",
        "\n",
        "def _get_before_dataset():\n",
        "  DATA_PATH = \"./data/\"\n",
        "\n",
        "  data = pd.read_csv(DATA_PATH + 'Crawling_Lyrics.csv')\n",
        "  data.dropna(axis=0, inplace = True)\n",
        "  song_lyrics = list(data['lyrics'])\n",
        "\n",
        "  lyrics = []\n",
        "  for song_lyric in song_lyrics:\n",
        "    sentences = []\n",
        "    for sent in song_lyric.split('\\n'):\n",
        "      if sent == '':\n",
        "        continue\n",
        "      sent = re.sub(r\"[^ㄱ-힣a-zA-Z0-9]+\", ' ', sent)\n",
        "      if sent:\n",
        "        sent = analyzer.pos(sent)\n",
        "      sentences.append(sent)\n",
        "    lyrics.append(sentences)\n",
        "  print(lyrics)\n",
        "\n",
        "  _save_pickle(\"./data/pk_before_input_data.pkl\", lyrics)\n",
        "\n",
        "\n",
        "def create_sequence(seq_length, lyrics):\n",
        "  data = list()\n",
        "  for lyric in lyrics:\n",
        "    for sent in lyric:\n",
        "      seq_data = list()\n",
        "\n",
        "      # seq_data 개수가 seq_length가 될때 까지          \n",
        "      for i in range(seq_length):\n",
        "        if i >= len(sent):\n",
        "          seq_data.append(('UNK' , ''))\n",
        "        else:\n",
        "          seq_data.append(sent[i])\n",
        "    data.append(seq_data)\n",
        "\n",
        "  _save_pickle(\"./data/2_pk_preprocessed_data.pkl\", data)\n",
        "\n",
        "  f = open('./data/2_pk_preprocessed_data.txt', 'w')\n",
        "  for tokens in data:\n",
        "    for word in tokens:\n",
        "      word = str(word) + ' '\n",
        "      f.write(word)\n",
        "    f.write('\\n')\n",
        "  f.close()\n",
        "\n",
        "def data_to_index(dataset, pos2idx):\n",
        "    idx_dataset = list()\n",
        "    for sent in dataset:\n",
        "        idx_sentence = list()\n",
        "        for word in sent:\n",
        "            idx_sentence.append(pos2idx[word[0]])\n",
        "        idx_dataset.append(idx_sentence)\n",
        "\n",
        "    _save_pickle(\"./data/3_pk_data_index.pkl\", idx_dataset)\n",
        "\n",
        "    # save pk_data_index.txt\n",
        "    f = open('./data/3_pk_data_index.txt', 'w')\n",
        "    for idx_sent in idx_dataset:\n",
        "        for word in idx_sent:\n",
        "            word = str(word) + ' '\n",
        "            f.write(word)\n",
        "        f.write('\\n')\n",
        "    f.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    seq_length = 30  # max 52\n",
        "    analyzer = Twitter()\n",
        "\n",
        "    # 이미 pkl 만들었으면 주석 처리, 처음 사용시 주석 해제\n",
        "    _get_before_dataset()\n",
        "\n",
        "    # load before dataset\n",
        "    a = open(\"./data/pk_before_input_data.pkl\", 'rb')\n",
        "    lyrics = pickle.load(a)\n",
        "\n",
        "    print(\"Create Sequence in a length of seq_length...\")\n",
        "    create_sequence(seq_length, lyrics)\n",
        "\n",
        "    print(\"Complete Creating sequence !!\")\n",
        "\n",
        "    # load after dataset\n",
        "    a = open(\"./data/2_pk_preprocessed_data.pkl\", 'rb')\n",
        "    dataset = pickle.load(a)\n",
        "\n",
        "    # load pos to index\n",
        "    a = open(\"./data/pk_pos2idx.pkl\", 'rb')\n",
        "    pos2idx = pickle.load(a)\n",
        "\n",
        "    print(\"Replace Sequence to Index...\")\n",
        "    data_to_index(dataset, pos2idx)\n",
        "\n",
        "    print(\"Complete Creating sequence to index !!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ax_4APlTQPaT"
      },
      "source": [
        "## create word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbVr-uYmQRKh"
      },
      "source": [
        "def _save_pickle(path, data):\n",
        "    # save pkl\n",
        "    f = open(path, 'wb')\n",
        "    pickle.dump(data, f)\n",
        "    f.close()\n",
        "\n",
        "\n",
        "# data 불러옴\n",
        "print(\"Load real data ...\")\n",
        "a = open('./data/1_pk_real_data.pkl', 'rb')\n",
        "sentences = pickle.load(a)\n",
        "\n",
        "# pk_idx2pos.pkl\n",
        "a = open('./data/pk_idx2pos.pkl', 'rb')\n",
        "idx2pos = pickle.load(a)\n",
        "\n",
        "# pk_pos2idx.pkl\n",
        "a = open('./data/pk_pos2idx.pkl', 'rb')\n",
        "pos2idx = pickle.load(a)\n",
        "\n",
        "sentence_idx = []\n",
        "for sentence in sentences:\n",
        "    words = []\n",
        "    for word in sentence:\n",
        "        words.append(pos2idx[word[0]])\n",
        "    sentence_idx.append(words)\n",
        "\n",
        "sentences_words = []\n",
        "for sen in sentence_idx:\n",
        "    sentence = []\n",
        "    for pos_idx in sen:\n",
        "        sentence.append(idx2pos[pos_idx])\n",
        "        sentences_words.append(sentence)\n",
        "\n",
        "# word2vec 학습\n",
        "print(\"Training word2vec ...\")\n",
        "model = Word2Vec(sentences_words, size=30, window=5,min_count=0, workers=4, iter=10, sg=1)\n",
        "\n",
        "# word2vec 테스트\n",
        "print(\"Test word2vec ...\")\n",
        "print(model.most_similar(\"불\"))\n",
        "\n",
        "# word2vec에 <start>, UNK 등 추가 후 numpy로 저장\n",
        "key = list(pos2idx.keys())\n",
        "w2v = []\n",
        "for k in key:\n",
        "    if k == '<start>' or k == 'UNK' or k == '후다':\n",
        "        print(k)\n",
        "        w2v.append(np.random.randn(30)*0.1)\n",
        "    else:\n",
        "        w2v.append(model.wv[k])\n",
        "\n",
        "w2v=np.array(w2v)\n",
        "\n",
        "_save_pickle('./data/pk_embedding_vec.pkl', w2v)\n",
        "\n",
        "print(\"Save word2vec !\")\n",
        "\n",
        "# pk_embedding_vec.pkl\n",
        "\n",
        "a = open('./data/pk_embedding_vec.pkl', 'rb')\n",
        "w2v_load = pickle.load(a)\n",
        "\n",
        "print(np.shape(w2v_load))\n",
        "print(w2v_load)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl1SxA6k_YqT"
      },
      "source": [
        "## 4. Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhQokd5v_KwW"
      },
      "source": [
        "class Gen_Data_loader():\n",
        "    def __init__(self, batch_size, sen_length):\n",
        "        self.batch_size = batch_size\n",
        "        self.token_stream = []\n",
        "        self.sen_length = sen_length\n",
        "\n",
        "    def create_batches(self, data_file):\n",
        "        self.token_stream = []\n",
        "        with open(data_file, 'r') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                line = line.split()\n",
        "                parse_line = [int(x) for x in line]\n",
        "                if len(parse_line) == self.sen_length:\n",
        "                    self.token_stream.append(parse_line)\n",
        "\n",
        "        self.num_batch = int(len(self.token_stream) / self.batch_size)\n",
        "        self.token_stream = self.token_stream[:self.num_batch * self.batch_size]\n",
        "        self.sequence_batch = np.split(np.array(self.token_stream), self.num_batch, 0)\n",
        "        self.pointer = 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        ret = self.sequence_batch[self.pointer]\n",
        "        self.pointer = (self.pointer + 1) % self.num_batch\n",
        "        return ret\n",
        "\n",
        "    def reset_pointer(self):\n",
        "        self.pointer = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4P1pFawY_QVp"
      },
      "source": [
        "class Dis_dataloader():\n",
        "    def __init__(self, batch_size, sen_length):\n",
        "        self.batch_size = batch_size\n",
        "        self.sentences = np.array([])\n",
        "        self.labels = np.array([])\n",
        "        self.sen_length = sen_length\n",
        "\n",
        "    def load_train_data(self, positive_file, negative_file):\n",
        "        # Load data\n",
        "        positive_examples = []\n",
        "        negative_examples = []\n",
        "        with open(positive_file)as fin:\n",
        "            for line in fin:\n",
        "                line = line.strip()\n",
        "                line = line.split()\n",
        "                parse_line = [int(x) for x in line]\n",
        "                positive_examples.append(parse_line)\n",
        "        with open(negative_file)as fin:\n",
        "            for line in fin:\n",
        "                line = line.strip()\n",
        "                line = line.split()\n",
        "                parse_line = [int(x) for x in line]\n",
        "                if len(parse_line) == self.sen_length:\n",
        "                    negative_examples.append(parse_line)\n",
        "        self.sentences = np.array(positive_examples + negative_examples)\n",
        "\n",
        "        # Generate labels\n",
        "        positive_labels = [[0, 1] for _ in positive_examples]\n",
        "        negative_labels = [[1, 0] for _ in negative_examples]\n",
        "        self.labels = np.concatenate([positive_labels, negative_labels], 0)\n",
        "\n",
        "        # Shuffle the data\n",
        "        shuffle_indices = np.random.permutation(np.arange(len(self.labels)))\n",
        "        self.sentences = self.sentences[shuffle_indices]\n",
        "        self.labels = self.labels[shuffle_indices]\n",
        "\n",
        "        # Split batches\n",
        "        self.num_batch = int(len(self.labels) / self.batch_size)\n",
        "        self.sentences = self.sentences[:self.num_batch * self.batch_size]\n",
        "        self.labels = self.labels[:self.num_batch * self.batch_size]\n",
        "        self.sentences_batches = np.split(self.sentences, self.num_batch, 0)\n",
        "        self.labels_batches = np.split(self.labels, self.num_batch, 0)\n",
        "\n",
        "        self.pointer = 0\n",
        "\n",
        "\n",
        "    def next_batch(self):\n",
        "        ret = self.sentences_batches[self.pointer], self.labels_batches[self.pointer]\n",
        "        self.pointer = (self.pointer + 1) % self.num_batch\n",
        "        return ret\n",
        "\n",
        "    def reset_pointer(self):\n",
        "        self.pointer = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIHYtPMe_cGY"
      },
      "source": [
        "## 5. Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6deYvocc_iQD"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    \"\"\" Generator \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, use_cuda):\n",
        "        super(Generator, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.use_cuda = use_cuda\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "        self.init_params()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Embeds input and applies LSTM on the input sequence.\n",
        "        Inputs: x\n",
        "            - x: (batch_size, seq_len), sequence of tokens generated by generator\n",
        "        Outputs: out\n",
        "            - out: (batch_size * seq_len, vocab_size), lstm output prediction\n",
        "        \"\"\"\n",
        "        self.lstm.flatten_parameters()\n",
        "        h0, c0 = self.init_hidden(x.size(0))\n",
        "        emb = self.embed(x) # batch_size * seq_len * emb_dim \n",
        "        out, _ = self.lstm(emb, (h0, c0)) # out: batch_size * seq_len * hidden_dim\n",
        "        out = self.log_softmax(self.fc(out.contiguous().view(-1, self.hidden_dim))) # (batch_size*seq_len) * vocab_size\n",
        "        return out\n",
        "\n",
        "    def step(self, x, h, c):\n",
        "        \"\"\"\n",
        "        Embeds input and applies LSTM one token at a time (seq_len = 1).\n",
        "        Inputs: x, h, c\n",
        "            - x: (batch_size, 1), sequence of tokens generated by generator\n",
        "            - h: (1, batch_size, hidden_dim), lstm hidden state\n",
        "            - c: (1, batch_size, hidden_dim), lstm cell state\n",
        "        Outputs: out, h, c\n",
        "            - out: (batch_size, vocab_size), lstm output prediction\n",
        "            - h: (1, batch_size, hidden_dim), lstm hidden state\n",
        "            - c: (1, batch_size, hidden_dim), lstm cell state \n",
        "        \"\"\"\n",
        "        self.lstm.flatten_parameters()\n",
        "        emb = self.embed(x) # batch_size * 1 * emb_dim\n",
        "        out, (h, c) = self.lstm(emb, (h, c)) # out: batch_size * 1 * hidden_dim\n",
        "        out = self.log_softmax(self.fc(out.contiguous().view(-1, self.hidden_dim))) # batch_size * vocab_size\n",
        "        return out, h, c\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        h = torch.zeros(1, batch_size, self.hidden_dim)\n",
        "        c = torch.zeros(1, batch_size, self.hidden_dim)\n",
        "        if self.use_cuda:\n",
        "            h, c = h.cuda(), c.cuda()\n",
        "        return h, c\n",
        "    \n",
        "    def init_params(self):\n",
        "        for param in self.parameters():\n",
        "            param.data.uniform_(-0.05, 0.05)\n",
        "\n",
        "    def sample(self, batch_size, seq_len, x=None):\n",
        "        \"\"\"\n",
        "        Samples the network and returns a batch of samples of length seq_len.\n",
        "        Outputs: out\n",
        "            - out: (batch_size * seq_len)\n",
        "        \"\"\"\n",
        "        samples = []\n",
        "        if x is None:\n",
        "            h, c = self.init_hidden(batch_size)\n",
        "            x = torch.zeros(batch_size, 1, dtype=torch.int64)\n",
        "            if self.use_cuda:\n",
        "                x = x.cuda()\n",
        "            for _ in range(seq_len):\n",
        "                out, h, c = self.step(x, h, c)\n",
        "                prob = torch.exp(out)\n",
        "                x = torch.multinomial(prob, 1)\n",
        "                samples.append(x)\n",
        "        else:\n",
        "            h, c = self.init_hidden(x.size(0))\n",
        "            given_len = x.size(1)\n",
        "            lis = x.chunk(x.size(1), dim=1)\n",
        "            for i in range(given_len):\n",
        "                out, h, c = self.step(lis[i], h, c)\n",
        "                samples.append(lis[i])\n",
        "            prob = torch.exp(out)\n",
        "            x = torch.multinomial(prob, 1)\n",
        "            for _ in range(given_len, seq_len):\n",
        "                samples.append(x)\n",
        "                out, h, c = self.step(x, h, c)\n",
        "                prob = torch.exp(out)\n",
        "                x = torch.multinomial(prob, 1)\n",
        "        out = torch.cat(samples, dim=1) # along the batch_size dimension\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cdRt324E7TA"
      },
      "source": [
        "## 6. Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKvcwbEZE-CO"
      },
      "source": [
        "\n",
        "# a = open('./data/pretrain_embedding_vec.pkl', 'rb')\n",
        "# word_embedding_matrix = pickle.load(a)\n",
        "# word_embedding_matrix = word_embedding_matrix.astype(np.float32)\n",
        "\n",
        "# An alternative to tf.nn.rnn_cell._linear function, which has been removed in Tensorfow 1.0.1\n",
        "# The highway layer is borrowed from https://github.com/mkroutikov/tf-lstm-char-cnn\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    A CNN for text classification.\n",
        "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
        "    Highway architecture based on the pooled feature maps is added. Dropout is adopted.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, vocab_size, embedding_dim, filter_sizes, num_filters, dropout_prob):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(1, num_f, (f_size, embedding_dim)) for f_size, num_f in zip(filter_sizes, num_filters)\n",
        "        ])\n",
        "        self.highway = nn.Linear(sum(num_filters), sum(num_filters))\n",
        "        self.dropout = nn.Dropout(p = dropout_prob)\n",
        "        self.fc = nn.Linear(sum(num_filters), num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Inputs: x\n",
        "            - x: (batch_size, seq_len)\n",
        "        Outputs: out\n",
        "            - out: (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        emb = self.embed(x).unsqueeze(1) # batch_size, 1 * seq_len * emb_dim\n",
        "        convs = [F.relu(conv(emb)).squeeze(3) for conv in self.convs] # [batch_size * num_filter * seq_len]\n",
        "        pools = [F.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in convs] # [batch_size * num_filter]\n",
        "        out = torch.cat(pools, 1)  # batch_size * sum(num_filters)\n",
        "        highway = self.highway(out)\n",
        "        transform = F.sigmoid(highway)\n",
        "        out = transform * F.relu(highway) + (1. - transform) * out # sets C = 1 - T\n",
        "        out = F.log_softmax(self.fc(self.dropout(out)), dim=1) # batch * num_classes\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31k0Dp6ZRap-"
      },
      "source": [
        "## target LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90mCsVA3RccD"
      },
      "source": [
        "\n",
        "class TargetLSTM(nn.Module):\n",
        "    \"\"\" Target LSTM \"\"\"\n",
        "\n",
        "    def __init__(self,  vocab_size, embedding_dim, hidden_dim, use_cuda):\n",
        "        super(TargetLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.use_cuda = use_cuda\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "        self.init_params()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Embeds input and applies LSTM on the input sequence.\n",
        "        Inputs: x\n",
        "            - x: (batch_size, seq_len), sequence of tokens generated by generator\n",
        "        Outputs: out\n",
        "            - out: (batch_size, vocab_size), lstm output prediction\n",
        "        \"\"\"\n",
        "        self.lstm.flatten_parameters()\n",
        "        h0, c0 = self.init_hidden(x.size(0))\n",
        "        emb = self.embed(x) # batch_size * seq_len * emb_dim \n",
        "        out, _ = self.lstm(emb, (h0, c0)) # out: seq_len * batch_size * hidden_dim\n",
        "        out = self.log_softmax(self.fc(out.contiguous().view(-1, self.hidden_dim))) # seq_len * batch_size * vocab_size\n",
        "        return out\n",
        "\n",
        "    def step(self, x, h, c):\n",
        "        \"\"\"\n",
        "        Embeds input and applies LSTM one token at a time (seq_len = 1).\n",
        "        Inputs: x, h, c\n",
        "            - x: (batch_size, 1), sequence of tokens generated by generator\n",
        "            - h: (1, batch_size, hidden_dim), lstm hidden state\n",
        "            - c: (1, batch_size, hidden_dim), lstm cell state\n",
        "        Outputs: out, h, c\n",
        "            - out: (batch_size, 1, vocab_size), lstm output prediction\n",
        "            - h: (1, batch_size, hidden_dim), lstm hidden state\n",
        "            - c: (1, batch_size, hidden_dim), lstm cell state \n",
        "        \"\"\"\n",
        "        self.lstm.flatten_parameters()\n",
        "        emb = self.embed(x) # batch_size * 1 * emb_dim\n",
        "        out, (h, c) = self.lstm(emb, (h, c)) # out: batch_size * 1 * hidden_dim\n",
        "        out = self.log_softmax(self.fc(out.contiguous().view(-1, self.hidden_dim))) # batch_size * vocab_size\n",
        "        return out, h, c\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        h = torch.zeros((1, batch_size, self.hidden_dim))\n",
        "        c = torch.zeros((1, batch_size, self.hidden_dim))\n",
        "        if self.use_cuda:\n",
        "            h, c = h.cuda(), c.cuda()\n",
        "        return h, c\n",
        "    \n",
        "    def init_params(self):\n",
        "        for param in self.parameters():\n",
        "            param.data.normal_(0, 1)\n",
        "\n",
        "    def sample(self, batch_size, seq_len):\n",
        "        \"\"\"\n",
        "        Samples the network and returns a batch of samples of length seq_len.\n",
        "        Outputs: out\n",
        "            - out: (batch_size * seq_len)\n",
        "        \"\"\"\n",
        "        samples = []\n",
        "        h, c = self.init_hidden(batch_size)\n",
        "        x = torch.zeros(batch_size, 1, dtype=torch.int64)\n",
        "        if self.use_cuda:\n",
        "            x = x.cuda()\n",
        "        for _ in range(seq_len):\n",
        "            out, h, c = self.step(x, h, c)\n",
        "            prob = torch.exp(out)\n",
        "            x = torch.multinomial(prob, 1)\n",
        "            samples.append(x)\n",
        "        out = torch.cat(samples, dim=1) # along the batch_size dimension\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB2j6Q8BRuje"
      },
      "source": [
        "## PGLoss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF5SjZL3RwBu"
      },
      "source": [
        "class PGLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Pseudo-loss that gives corresponding policy gradients (on calling .backward()) \n",
        "    for adversial training of Generator\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(PGLoss, self).__init__()\n",
        "\n",
        "    def forward(self, pred, target, reward):\n",
        "        \"\"\"\n",
        "        Inputs: pred, target, reward\n",
        "            - pred: (batch_size, seq_len), \n",
        "            - target : (batch_size, seq_len), \n",
        "            - reward : (batch_size, ), reward of each whole sentence\n",
        "        \"\"\"\n",
        "        one_hot = torch.zeros(pred.size(), dtype=torch.uint8)\n",
        "        if pred.is_cuda:\n",
        "            one_hot = one_hot.cuda()\n",
        "        one_hot.scatter_(1, target.data.view(-1, 1), 1)\n",
        "        loss = torch.masked_select(pred, one_hot)\n",
        "        loss = loss * reward.contiguous().view(-1)\n",
        "        loss = -torch.sum(loss)\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Fus3bO-SP1V"
      },
      "source": [
        "## data inter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIQCUQyfSRG6"
      },
      "source": [
        "\n",
        "class GenDataIter:\n",
        "    \"\"\" Toy data iter to load digits \"\"\"\n",
        "\n",
        "    def __init__(self, data_file, batch_size):\n",
        "        super(GenDataIter, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.data_lis = self.read_file(data_file)\n",
        "        self.data_num = len(self.data_lis)\n",
        "        self.indices = range(self.data_num)\n",
        "        self.num_batches = math.ceil(self.data_num / self.batch_size)\n",
        "        self.idx = 0\n",
        "        self.reset()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_batches\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        return self.next()\n",
        "    \n",
        "    def reset(self):\n",
        "        self.idx = 0\n",
        "        random.shuffle(self.data_lis)\n",
        "\n",
        "    def next(self):\n",
        "        if self.idx >= self.data_num:\n",
        "            raise StopIteration\n",
        "        index = self.indices[self.idx : self.idx + self.batch_size]\n",
        "        d = [self.data_lis[i] for i in index]\n",
        "        d = torch.tensor(d)\n",
        "\n",
        "        # 0 is prepended to d as start symbol\n",
        "        data = torch.cat([torch.zeros(len(index), 1, dtype=torch.int64), d], dim=1)\n",
        "        target = torch.cat([d, torch.zeros(len(index), 1, dtype=torch.int64)], dim=1)\n",
        "        \n",
        "        self.idx += self.batch_size\n",
        "        return data, target\n",
        "\n",
        "    def read_file(self, data_file):\n",
        "        with open(data_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        lis = []\n",
        "        for line in lines:\n",
        "            l = [int(s) for s in list(line.strip().split())]\n",
        "            lis.append(l)\n",
        "        return lis\n",
        "\n",
        "\n",
        "class DisDataIter:\n",
        "    \"\"\" Toy data iter to load digits \"\"\"\n",
        "\n",
        "    def __init__(self, real_data_file, fake_data_file, batch_size):\n",
        "        super(DisDataIter, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        real_data_lis = self.read_file(real_data_file)\n",
        "        fake_data_lis = self.read_file(fake_data_file)\n",
        "        self.data = real_data_lis + fake_data_lis\n",
        "        self.labels = [1 for _ in range(len(real_data_lis))] +\\\n",
        "                        [0 for _ in range(len(fake_data_lis))]\n",
        "        self.pairs = list(zip(self.data, self.labels))\n",
        "        self.data_num = len(self.pairs)\n",
        "        self.indices = range(self.data_num)\n",
        "        self.num_batches = math.ceil(self.data_num / self.batch_size)\n",
        "        self.idx = 0\n",
        "        self.reset()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_batches\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        return self.next()\n",
        "    \n",
        "    def reset(self):\n",
        "        self.idx = 0\n",
        "        random.shuffle(self.pairs)\n",
        "\n",
        "    def next(self):\n",
        "        if self.idx >= self.data_num:\n",
        "            raise StopIteration\n",
        "        index = self.indices[self.idx : self.idx + self.batch_size]\n",
        "        pairs = [self.pairs[i] for i in index]\n",
        "        data = [p[0] for p in pairs]\n",
        "        label = [p[1] for p in pairs]\n",
        "        data = torch.tensor(data)\n",
        "        label = torch.tensor(label)\n",
        "        self.idx += self.batch_size\n",
        "        return data, label\n",
        "\n",
        "    def read_file(self, data_file):\n",
        "        with open(data_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        lis = []\n",
        "        for line in lines:\n",
        "            l = [int(s) for s in list(line.strip().split())]\n",
        "            lis.append(l) \n",
        "        return lis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgbqe0LKVcLC"
      },
      "source": [
        "## roll out"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPazU1vmVdHU"
      },
      "source": [
        "class Rollout(object):\n",
        "    \"\"\" Rollout Policy \"\"\"\n",
        "\n",
        "    def __init__(self, model, update_rate):\n",
        "        self.ori_model = model\n",
        "        self.own_model = copy.deepcopy(model)\n",
        "        self.update_rate = update_rate\n",
        "\n",
        "    def get_reward(self, x, num, discriminator):\n",
        "        \"\"\"\n",
        "        Inputs: x, num, discriminator\n",
        "            - x: (batch_size, seq_len) input data\n",
        "            - num: rollout number\n",
        "            - discriminator: discrimanator model\n",
        "        \"\"\"\n",
        "        rewards = []\n",
        "        batch_size = x.size(0)\n",
        "        seq_len = x.size(1)\n",
        "        for i in range(num):\n",
        "            for l in range(1, seq_len):\n",
        "                data = x[:, 0:l]\n",
        "                samples = self.own_model.sample(batch_size, seq_len, data)\n",
        "                pred = discriminator(samples)\n",
        "                pred = pred.cpu().data[:,1].numpy()\n",
        "                if i == 0:\n",
        "                    rewards.append(pred)\n",
        "                else:\n",
        "                    rewards[l-1] += pred\n",
        "\n",
        "            # for the last token\n",
        "            pred = discriminator(x)\n",
        "            pred = pred.cpu().data[:, 1].numpy()\n",
        "            if i == 0:\n",
        "                rewards.append(pred)\n",
        "            else:\n",
        "                rewards[seq_len-1] += pred\n",
        "        rewards = np.transpose(np.array(rewards)) / (1.0 * num) # batch_size * seq_len\n",
        "        return rewards\n",
        "\n",
        "    def update_params(self):\n",
        "        dic = {}\n",
        "        for name, param in self.ori_model.named_parameters():\n",
        "            dic[name] = param.data\n",
        "        for name, param in self.own_model.named_parameters():\n",
        "            if name.startswith('emb'):\n",
        "                param.data = dic[name]\n",
        "            else:\n",
        "                param.data = self.update_rate * param.data + (1 - self.update_rate) * dic[name]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DXaHb2CchoW"
      },
      "source": [
        "## 4. sequence_gan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0XyNKotcDqW"
      },
      "source": [
        "# Arguemnts\n",
        "parser = argparse.ArgumentParser(description='SeqGAN')\n",
        "parser.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")\n",
        "parser.add_argument('--hpc', action='store_true', default=False,\n",
        "                    help='set to hpc mode')\n",
        "parser.add_argument('--data_path', type=str, default='/content/data/', metavar='PATH',\n",
        "                    help='data path to save files (default: /content/data/)')\n",
        "parser.add_argument('--rounds', type=int, default=150, metavar='N',\n",
        "                    help='rounds of adversarial training (default: 150)')\n",
        "parser.add_argument('--g_pretrain_steps', type=int, default=120, metavar='N',\n",
        "                    help='steps of pre-training of generators (default: 120)')\n",
        "parser.add_argument('--d_pretrain_steps', type=int, default=50, metavar='N',\n",
        "                    help='steps of pre-training of discriminators (default: 50)')\n",
        "parser.add_argument('--g_steps', type=int, default=1, metavar='N',\n",
        "                    help='steps of generator updates in one round of adverarial training (default: 1)')\n",
        "parser.add_argument('--d_steps', type=int, default=3, metavar='N',\n",
        "                    help='steps of discriminator updates in one round of adverarial training (default: 3)')\n",
        "parser.add_argument('--gk_epochs', type=int, default=1, metavar='N',\n",
        "                    help='epochs of generator updates in one step of generate update (default: 1)')\n",
        "parser.add_argument('--dk_epochs', type=int, default=3, metavar='N',\n",
        "                    help='epochs of discriminator updates in one step of discriminator update (default: 3)')\n",
        "parser.add_argument('--update_rate', type=float, default=0.8, metavar='UR',\n",
        "                    help='update rate of roll-out model (default: 0.8)')\n",
        "parser.add_argument('--n_rollout', type=int, default=16, metavar='N',\n",
        "                    help='number of roll-out (default: 16)')\n",
        "parser.add_argument('--vocab_size', type=int, default=10, metavar='N',\n",
        "                    help='vocabulary size (default: 10)')\n",
        "parser.add_argument('--batch_size', type=int, default=64, metavar='N',\n",
        "                    help='batch size (default: 64)')\n",
        "parser.add_argument('--n_samples', type=int, default=6400, metavar='N',\n",
        "                    help='number of samples gerenated per time (default: 6400)')\n",
        "parser.add_argument('--gen_lr', type=float, default=1e-3, metavar='LR',\n",
        "                    help='learning rate of generator optimizer (default: 1e-3)')\n",
        "parser.add_argument('--dis_lr', type=float, default=1e-3, metavar='LR',\n",
        "                    help='learning rate of discriminator optimizer (default: 1e-3)')\n",
        "parser.add_argument('--no_cuda', action='store_true', default=False,\n",
        "                    help='disables CUDA training')\n",
        "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
        "                    help='random seed (default: 1)')\n",
        "\n",
        "\n",
        "#########################################################################################\n",
        "#  Generator  Hyper-parameters\n",
        "######################################################################################\n",
        "g_embed_dim = 30 # embedding dimension (pretrained: 200, pk: 30)\n",
        "g_hidden_dim = 300 # hidden state dimension of lstm cell\n",
        "g_seq_len = 30 # sequence length\n",
        "START_TOKEN = 0\n",
        "PRE_EPOCH_NUM = 120  # supervise (maximum likelihood estimation) epochs\n",
        "SEED = 88\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "#########################################################################################\n",
        "#  Discriminator  Hyper-parameters\n",
        "#########################################################################################\n",
        "d_num_class = 2\n",
        "d_embed_dim = EMB_DIM\n",
        "d_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 30]\n",
        "d_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]\n",
        "d_dropout_prob = 0.75\n",
        "dis_l2_reg_lambda = 0.2\n",
        "dis_batch_size = 64\n",
        "\n",
        "#########################################################################################\n",
        "#  Basic Training Parameters\n",
        "#########################################################################################\n",
        "TOTAL_BATCH = 200\n",
        "generated_num = 100\n",
        "sample_num = 10\n",
        "\n",
        "# original seqgan parameter\n",
        "# HIDDEN_DIM = 32\n",
        "# PRE_EPOCH_NUM = 120\n",
        "# TOTAL_BATCH = 200\n",
        "# generated_num = 10000\n",
        "\n",
        "POSITIVE_FILE = './data/3_pk_data_index.txt'\n",
        "NEGATIVE_FILE = 'save/negative_sample.txt'\n",
        "eval_file = 'save/eval_file.txt'\n",
        "# \"pretrain\" or \"poke\"\n",
        "embed_flag = \"poke\"\n",
        "\n",
        "a = open('./data/3_pk_data_index.pkl', 'rb')\n",
        "real_data = pickle.load(a)\n",
        "\n",
        "a = open('./data/pk_pos2idx.pkl', 'rb')\n",
        "vocab_to_int = pickle.load(a)\n",
        "\n",
        "a = open('./data/pk_idx2pos.pkl', 'rb')\n",
        "int_to_vocab = pickle.load(a)\n",
        "print(int_to_vocab)\n",
        "\n",
        "if embed_flag == \"pretrain\":\n",
        "    a = open('./data/pretrain_embedding_vec.pkl', 'rb')\n",
        "elif embed_flag == \"poke\":\n",
        "    a = open('./data/pk_embedding_vec.pkl', 'rb')\n",
        "word_embedding_matrix = pickle.load(a)\n",
        "word_embedding_matrix = word_embedding_matrix.astype(np.float32)\n",
        "\n",
        "# a = open('./data/word_dict.pickle', 'rb')\n",
        "# word_dict = pickle.load(a)\n",
        "\n",
        "real_data_vocab = [[int_to_vocab[i] for i in sample if int_to_vocab[i] != '<PAD>'] for sample in real_data]\n",
        "real_data_vocab = [' '.join(sample) for sample in real_data_vocab]\n",
        "print(len(real_data_vocab))\n",
        "\n",
        "\n",
        "def generate_samples(model, batch_size, generated_num, output_file):\n",
        "    samples = []\n",
        "    for _ in range(int(generated_num / batch_size)):\n",
        "        sample = model.sample(batch_size, g_seq_len).cpu().data.numpy().tolist()\n",
        "        samples.extend(sample)\n",
        "    with open(output_file, 'w') as fout:\n",
        "        for sample in samples:\n",
        "            string = ' '.join([str(s) for s in sample])\n",
        "            fout.write('{}\\n'.format(string))\n",
        "\n",
        "\n",
        "def train_generator_MLE(gen, data_iter, criterion, optimizer, epochs, \n",
        "        gen_pretrain_train_loss, args):\n",
        "    \"\"\"\n",
        "    Train generator with MLE\n",
        "    \"\"\"\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.\n",
        "        for data, target in data_iter:\n",
        "            if args.cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            target = target.contiguous().view(-1)\n",
        "            output = gen(data)\n",
        "            loss = criterion(output, target)\n",
        "            total_loss += loss.item()\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        data_iter.reset()\n",
        "    avg_loss = total_loss / len(data_iter)\n",
        "    print(\"Epoch {}, train loss: {:.5f}\".format(epoch, avg_loss))\n",
        "    gen_pretrain_train_loss.append(avg_loss)\n",
        "\n",
        "def train_generator_PG(gen, dis, rollout, pg_loss, optimizer, epochs, args):\n",
        "    \"\"\"\n",
        "    Train generator with the guidance of policy gradient\n",
        "    \"\"\"\n",
        "    for epoch in range(epochs):\n",
        "        # construct the input to the genrator, add zeros before samples and delete the last column\n",
        "        samples = generator.sample(args.batch_size, g_seq_len)\n",
        "        zeros = torch.zeros(args.batch_size, 1, dtype=torch.int64)\n",
        "        if samples.is_cuda:\n",
        "            zeros = zeros.cuda()\n",
        "        inputs = torch.cat([zeros, samples.data], dim = 1)[:, :-1].contiguous()\n",
        "        targets = samples.data.contiguous().view((-1,))\n",
        "\n",
        "        # calculate the reward\n",
        "        rewards = torch.tensor(rollout.get_reward(samples, args.n_rollout, dis))\n",
        "        if args.cuda:\n",
        "            rewards = rewards.cuda()\n",
        "\n",
        "        # update generator\n",
        "        output = gen(inputs)\n",
        "        loss = pg_loss(output, targets, rewards)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def eval_generator(model, data_iter, criterion, args):\n",
        "    \"\"\"\n",
        "    Evaluate generator with NLL\n",
        "    \"\"\"\n",
        "    total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_iter:\n",
        "            if args.cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            target = target.contiguous().view(-1)\n",
        "            pred = model(data)\n",
        "            loss = criterion(pred, target)\n",
        "            total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(data_iter)\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def train_discriminator(dis, gen, criterion, optimizer, epochs, \n",
        "        dis_adversarial_train_loss, dis_adversarial_train_acc, args):\n",
        "    \"\"\"\n",
        "    Train discriminator\n",
        "    \"\"\"\n",
        "    generate_samples(gen, args.batch_size, args.n_samples, NEGATIVE_FILE)\n",
        "    data_iter = DisDataIter(POSITIVE_FILE, NEGATIVE_FILE, args.batch_size)\n",
        "    for epoch in range(epochs):\n",
        "        correct = 0\n",
        "        total_loss = 0.\n",
        "        for data, target in data_iter:\n",
        "            if args.cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            target = target.contiguous().view(-1)\n",
        "            output = dis(data)\n",
        "            pred = output.data.max(1)[1]\n",
        "            correct += pred.eq(target.data).cpu().sum()\n",
        "            loss = criterion(output, target)\n",
        "            total_loss += loss.item()\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        data_iter.reset()\n",
        "        avg_loss = total_loss / len(data_iter)\n",
        "        acc = correct.item() / data_iter.data_num\n",
        "        print(\"Epoch {}, train loss: {:.5f}, train acc: {:.3f}\".format(epoch, avg_loss, acc))\n",
        "        dis_adversarial_train_loss.append(avg_loss)\n",
        "        dis_adversarial_train_acc.append(acc)\n",
        "\n",
        "\n",
        "def eval_discriminator(model, data_iter, criterion, args):\n",
        "    \"\"\"\n",
        "    Evaluate discriminator, dropout is enabled\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_iter:\n",
        "            if args.cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            target = target.contiguous().view(-1)\n",
        "            output = model(data)\n",
        "            pred = output.data.max(1)[1]\n",
        "            correct += pred.eq(target.data).cpu().sum()\n",
        "            loss = criterion(output, target)\n",
        "            total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(data_iter)\n",
        "    acc = correct.item() / data_iter.data_num\n",
        "    return avg_loss, acc\n",
        "\n",
        "\n",
        "def adversarial_train(gen, dis, rollout, pg_loss, nll_loss, gen_optimizer, dis_optimizer, \n",
        "        dis_adversarial_train_loss, dis_adversarial_train_acc, args):\n",
        "    \"\"\"\n",
        "    Adversarially train generator and discriminator\n",
        "    \"\"\"\n",
        "    # train generator for g_steps\n",
        "    print(\"#Train generator\")\n",
        "    for i in range(args.g_steps):\n",
        "        print(\"##G-Step {}\".format(i))\n",
        "        train_generator_PG(gen, dis, rollout, pg_loss, gen_optimizer, args.gk_epochs, args)\n",
        "\n",
        "    # train discriminator for d_steps\n",
        "    print(\"#Train discriminator\")\n",
        "    for i in range(args.d_steps):\n",
        "        print(\"##D-Step {}\".format(i))\n",
        "        train_discriminator(dis, gen, nll_loss, dis_optimizer, args.dk_epochs, \n",
        "            dis_adversarial_train_loss, dis_adversarial_train_acc, args)\n",
        "\n",
        "    # update roll-out model\n",
        "    rollout.update_params()\n",
        "\n",
        "\n",
        "def make_sample(eval_file, sample_num):\n",
        "    samples = []\n",
        "    with open(eval_file, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            line = line.split()\n",
        "            parse_line = [x for x in line]\n",
        "            samples.append(parse_line)\n",
        "\n",
        "    sample_vocab = samples\n",
        "\n",
        "    return sample_vocab\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Parse arguments\n",
        "    args = parser.parse_args()\n",
        "    args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.cuda:\n",
        "        torch.cuda.manual_seed(args.seed)\n",
        "    if not args.hpc:\n",
        "        args.data_path = ''\n",
        "    POSITIVE_FILE = args.data_path + POSITIVE_FILE\n",
        "    NEGATIVE_FILE = args.data_path + NEGATIVE_FILE\n",
        "\n",
        "    # Set models, criteria, optimizers\n",
        "    generator = Generator(args.vocab_size, g_embed_dim, g_hidden_dim, args.cuda)\n",
        "    discriminator = Discriminator(d_num_class, args.vocab_size, d_embed_dim, d_filter_sizes, d_num_filters, d_dropout_prob)\n",
        "    target_lstm = TargetLSTM(args.vocab_size, g_embed_dim, g_hidden_dim, args.cuda)\n",
        "    nll_loss = nn.NLLLoss()\n",
        "    pg_loss = PGLoss()\n",
        "    if args.cuda:\n",
        "        generator = generator.cuda()\n",
        "        discriminator = discriminator.cuda()\n",
        "        target_lstm = target_lstm.cuda()\n",
        "        nll_loss = nll_loss.cuda()\n",
        "        pg_loss = pg_loss.cuda()\n",
        "        cudnn.benchmark = True\n",
        "    gen_optimizer = optim.Adam(params=generator.parameters(), lr=args.gen_lr)\n",
        "    dis_optimizer = optim.SGD(params=discriminator.parameters(), lr=args.dis_lr)\n",
        "\n",
        "    # Container of experiment data\n",
        "    gen_pretrain_train_loss = []\n",
        "    gen_pretrain_eval_loss = []\n",
        "    dis_pretrain_train_loss = []\n",
        "    dis_pretrain_train_acc = []\n",
        "    dis_pretrain_eval_loss = []\n",
        "    dis_pretrain_eval_acc = []\n",
        "    gen_adversarial_eval_loss = []\n",
        "    dis_adversarial_train_loss = []\n",
        "    dis_adversarial_train_acc = []\n",
        "    dis_adversarial_eval_loss = []\n",
        "    dis_adversarial_eval_acc = []\n",
        "\n",
        "    # Generate toy data using target LSTM\n",
        "    print('#####################################################')\n",
        "    print('Generating data ...')\n",
        "    print('#####################################################\\n\\n')\n",
        "    generate_samples(target_lstm, args.batch_size, args.n_samples, POSITIVE_FILE)\n",
        "\n",
        "    # Pre-train generator using MLE\n",
        "    print('#####################################################')\n",
        "    print('Start pre-training generator with MLE...')\n",
        "    print('#####################################################\\n')\n",
        "    gen_data_iter = GenDataIter(POSITIVE_FILE, args.batch_size)\n",
        "    # for i in range(args.g_pretrain_steps):\n",
        "    for i in range(1):      \n",
        "        print(\"G-Step {}\".format(i))\n",
        "        train_generator_MLE(generator, gen_data_iter, nll_loss, \n",
        "            gen_optimizer, args.gk_epochs, \n",
        "            gen_pretrain_train_loss, args)\n",
        "        generate_samples(generator, args.batch_size, args.n_samples, NEGATIVE_FILE)\n",
        "        eval_iter = GenDataIter(NEGATIVE_FILE, args.batch_size)\n",
        "        gen_loss = eval_generator(target_lstm, eval_iter, nll_loss, args)\n",
        "        gen_pretrain_eval_loss.append(gen_loss)\n",
        "        print(\"eval loss: {:.5f}\\n\".format(gen_loss))\n",
        "    print('#####################################################\\n\\n')\n",
        "\n",
        "    # Pre-train discriminator\n",
        "    print('#####################################################')\n",
        "    print('Start pre-training discriminator...')\n",
        "    print('#####################################################\\n')\n",
        "    for i in range(1):\n",
        "    # for i in range(args.d_pretrain_steps):      \n",
        "        print(\"D-Step {}\".format(i))\n",
        "        train_discriminator(discriminator, generator, nll_loss, \n",
        "            dis_optimizer, args.dk_epochs, \n",
        "            dis_adversarial_train_loss, dis_adversarial_train_acc, args)\n",
        "        generate_samples(generator, args.batch_size, args.n_samples, NEGATIVE_FILE)\n",
        "        eval_iter = DisDataIter(POSITIVE_FILE, NEGATIVE_FILE, args.batch_size)\n",
        "        dis_loss, dis_acc = eval_discriminator(discriminator, eval_iter, nll_loss, args)\n",
        "        dis_pretrain_eval_loss.append(dis_loss)\n",
        "        dis_pretrain_eval_acc.append(dis_acc)\n",
        "        print(\"eval loss: {:.5f}, eval acc: {:.3f}\\n\".format(dis_loss, dis_acc))\n",
        "    print('#####################################################\\n\\n')\n",
        "\n",
        "    # Adversarial training\n",
        "    print('#####################################################')\n",
        "    print('Start adversarial training...')\n",
        "    print('#####################################################\\n')\n",
        "    rollout = Rollout(generator, args.update_rate)\n",
        "    for i in range(1):    \n",
        "    # for i in range(args.rounds):\n",
        "        print(\"Round {}\".format(i))\n",
        "        adversarial_train(generator, discriminator, rollout, \n",
        "            pg_loss, nll_loss, gen_optimizer, dis_optimizer, \n",
        "            dis_adversarial_train_loss, dis_adversarial_train_acc, args)\n",
        "        generate_samples(generator, args.batch_size, args.n_samples, NEGATIVE_FILE)\n",
        "        gen_eval_iter = GenDataIter(NEGATIVE_FILE, args.batch_size)\n",
        "        dis_eval_iter = DisDataIter(POSITIVE_FILE, NEGATIVE_FILE, args.batch_size)\n",
        "        gen_loss = eval_generator(target_lstm, gen_eval_iter, nll_loss, args)\n",
        "        gen_adversarial_eval_loss.append(gen_loss)\n",
        "        dis_loss, dis_acc = eval_discriminator(discriminator, dis_eval_iter, nll_loss, args)\n",
        "        dis_adversarial_eval_loss.append(dis_loss)\n",
        "        dis_adversarial_eval_acc.append(dis_acc)\n",
        "        print(\"gen eval loss: {:.5f}, dis eval loss: {:.5f}, dis eval acc: {:.3f}\\n\"\n",
        "            .format(gen_loss, dis_loss, dis_acc))\n",
        "\n",
        "    # Save experiment data\n",
        "    with open(args.data_path + 'experiment.pkl', 'wb') as f:\n",
        "        pickle.dump(\n",
        "            (gen_pretrain_train_loss,\n",
        "                gen_pretrain_eval_loss,\n",
        "                dis_pretrain_train_loss,\n",
        "                dis_pretrain_train_acc,\n",
        "                dis_pretrain_eval_loss,\n",
        "                dis_pretrain_eval_acc,\n",
        "                gen_adversarial_eval_loss,\n",
        "                dis_adversarial_train_loss,\n",
        "                dis_adversarial_train_acc,\n",
        "                dis_adversarial_eval_loss,\n",
        "                dis_adversarial_eval_acc),\n",
        "            f,\n",
        "            protocol=pickle.HIGHEST_PROTOCOL\n",
        "        )\n",
        "\n",
        "\n",
        "    samples = make_sample(eval_file, generated_num)\n",
        "    samples = [[word for word in sample.split() if word != 'UNK'] for sample in samples]\n",
        "    samples = [' '.join(sample) for sample in samples]\n",
        "\n",
        "    f = open('./save/final_output_vocab.txt', 'w')\n",
        "    for token in samples:\n",
        "        token = token + '\\n'\n",
        "        f.write(token)\n",
        "    f.close()\n",
        "\n",
        "\n",
        "\n",
        "    a = open('./data/pk_embedding_vec.pkl', 'rb')\n",
        "    w2v_load = pickle.load(a)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvZeK5oFfNLF"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_8zJYFnRfum"
      },
      "source": [
        "samples = []\n",
        "with open(eval_file, 'r') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        line = line.split()\n",
        "        parse_line = [x for x in line]\n",
        "        print(parse_line)\n",
        "\n",
        "        for word in parse_line:\n",
        "          if word in vocab_to_int.keys():\n",
        "            words = vocab_to_int[word]\n",
        "            samples.append(words)\n",
        "\n",
        "sample_int = samples[:20]\n",
        "sample_vocab = [int_to_vocab[sample] for sample in sample_int]\n",
        "sample_vocab = [''.join(sample) for sample in sample_vocab]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTxZdtH70KkP"
      },
      "source": [
        "sample_int"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8TUk7Ai0ccY"
      },
      "source": [
        "sample_vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSEksmG7epat"
      },
      "source": [
        "a = open('./data/pk_pos2idx.pkl', 'rb')\n",
        "vocab_to_int = pickle.load(a)\n",
        "\n",
        "vocab_to_int['나도']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-3LHU6RxMIz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}