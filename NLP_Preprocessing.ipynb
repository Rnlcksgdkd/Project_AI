{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO3+lr3ACvvSnmSrUWRRDQU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rnlcksgdkd/Project_AI/blob/ando/NLP_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lijF7c9wRQhW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShUSxQ4EAxYu"
      },
      "source": [
        "# **I. 텍스트 전처리**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SziUm4veIqkV"
      },
      "source": [
        "> ## **형태소 분석기 어간추출**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO-xKC4WIqEr"
      },
      "source": [
        "import okt\n",
        "okt = Okt()\n",
        "okt.morphs(text , stem = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g_yWRZ53dwE"
      },
      "source": [
        "> ## **최대단어수 및 최대단어길이 제한**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTH_XMYN3apO"
      },
      "source": [
        "sentences_new = []\n",
        "\n",
        "for sentence in X:\n",
        "  sentences_new.append( [ word[:10] for word in sentence[:30]] )\n",
        "X = sentences_new\n",
        "\n",
        "for i in range(5): print(X[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clvkS4nmAbvJ"
      },
      "source": [
        "> ## **html 태그 삭제**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vgRRgg0Aele"
      },
      "source": [
        "I : input\n",
        "O : output\n",
        "\n",
        "import BeautifulSoup\n",
        "output = BeautifulSoup(input , 'html5lib').get_text()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7dA_dQS84Ci"
      },
      "source": [
        "> ## **정규식 이용**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWFZg5F69cwc"
      },
      "source": [
        "# 한글용 문자정리함수\n",
        "def clean_str(string):\n",
        "\n",
        "    string = re.sub(r\"[^가-힣A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    string = re.sub(r\"\\'{2,}\", \"\\'\", string)\n",
        "    string = re.sub(r\"\\'\", \"\", string)\n",
        "\n",
        "    return string.lower()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwztF_wD8-SX"
      },
      "source": [
        "\n",
        "# 한글 제외 정리\n",
        "def clean_hangle(string):\n",
        "  output = re.sub(\"[가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]\" , \"\" , input )\n",
        "  return output\n",
        "\n",
        "# 영어 제외 정리\n",
        "def clean_english(string):\n",
        "  output = re.sub(\"[^a-zA-Z]\" , \" \" , input)\n",
        "  return output\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nflzZSE-FmV"
      },
      "source": [
        "> ## **불용어 사전 정의**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYyeA7Fg-Sl0"
      },
      "source": [
        "\n",
        "# 영어 불용어 정의 (nltk 에서 정의된 불용어)\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cj4zOPEV43Zv"
      },
      "source": [
        "\n",
        "# 한글 불용어 정의\n",
        "\n",
        "stop_words = ['은', '는' , '이' , '가' , '하', '아', '것' , '들', '의', '있' , '되' , '수' , '보' , '주' , '등' ,'한' ]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cYu0Y3DMfZJ"
      },
      "source": [
        "> ## **전처리 함수**\n",
        "  - re 모듈 이용한 문자정리\n",
        "  - okt 형태소 추출\n",
        "  - 불용어 처리  \n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOCoKCY6MiNA"
      },
      "source": [
        "\n",
        "def preprocessing(text , okt , limit_words  , stop_words , print_option = False ):\n",
        "\n",
        "  if print_option : print(\"원본\".ljust(15 , ' ') + \": \" , text)\n",
        "  \n",
        "  text = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]\" , \"\" , text)\n",
        "  if print_option : print(\"정규표현식 처리\".ljust(15 , ' ') + \": \" , text)\n",
        "\n",
        "  max_word = limit_words[0]\n",
        "  max_length = limit_words[1]\n",
        "  text = [word[:max_length] for word in sentence[:max_word]]\n",
        "  if print_option : print(\"문장 길이 조절\".ljust(15 , ' ') + \": \" , text)\n",
        "\n",
        "  \n",
        "  text = okt.morphs(text , stem = True)\n",
        "  if print_option : print(\"okt 형태소 추출\".ljust(15 , ' ') + \": \" , text)\n",
        "\n",
        "  text = [token for token in text if not token in stop_words]\n",
        "  if print_option : print(\"불용어 처리\".ljust(15 , ' ') + \": \" , text)\n",
        "\n",
        "  \n",
        "\n",
        "  print(\" \")\n",
        "  print(\" \")\n",
        "\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_cCZi72A_Wk"
      },
      "source": [
        "# **II. Tokenizer/Vectorizer**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W14FAf9N34qo"
      },
      "source": [
        "> ## **Keras Tokenizer/Padding 이용**\n",
        "  - 말뭉치 (sentences) , num_words (단어사전 길이)  \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcEsE45dBHJK"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def Tokenize_Padding(sentences , num_words = 20000):\n",
        "  tokenizer = Tokenizer(num_words = 20000)\n",
        "  tokenizer.fit_on_texts(sentences)\n",
        "  X = tokenizer.texts_to_sequences(sentences)\n",
        "  X = pad_sequences(train_x , padding = 'post')\n",
        "  return tokenizer , X\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVI5tS8URyk7"
      },
      "source": [
        "\n",
        "tokenizer = fit_tokenizer(20000 ,clean_text)\n",
        "\n",
        "train_inputs = Tokenize_Padding(tokenizer , clean_text , 15 ,  'post')\n",
        "test_inputs = Tokenize_Padding(tokenizer , clean_text_test , 15 , 'post')\n",
        "\n",
        "train_labels = np.array(train_df['label'])\n",
        "test_labels = np.array(test_df['label'])\n",
        "\n",
        "train_inputs.shape , test_inputs.shape , train_labels.shape , test_labels.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6kwstHhRy8I"
      },
      "source": [
        "# III. **Data Save**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZQYvSS2R2Cl"
      },
      "source": [
        "import json\n",
        "\n",
        "data_path = \"/content/Project_AI/ando/NaverMovie/\"\n",
        "train_df.to_csv(data_path + \"Naver_MR_train.csv\" , index = False)\n",
        "test_df.to_csv(data_path +\"Naver_MR_test.csv\" , index = False)\n",
        "\n",
        "np.save(data_path +\"Naver_MR_train_input.npy\" , train_inputs)\n",
        "np.save(data_path +\"Naver_MR_test_input.npy\" , train_inputs)\n",
        "np.save(data_path + \"Naver_MR_train_label.npy\" , train_labels)\n",
        "np.save(data_path + \"Naver_MR_test_label.npy\" , train_labels)\n",
        "\n",
        "data_configs = {}\n",
        "\n",
        "data_configs['vocab'] = word_vocab\n",
        "data_configs['vocab_size'] = len(word_vocab) + 1\n",
        "\n",
        "json.dump(data_configs , open(data_path + \"data_config.json\" , 'w') , ensure_ascii=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}